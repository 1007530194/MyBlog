
<!DOCTYPE html>
<html lang="zh">
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
        <meta name="author" content="niult" />
        <meta name="keywords" content="TensorFlowExample" />
        <meta name="description" content="Word2Vec (Word Embedding)¶Implement Word2Vec algorithm to compute vector representations of words. This example is using a small chunk of Wikipedia articles to train from. More info: Mikolov, Tomas et al. &#34;Efficient Estimation of Word Representations in Vector Space.&#34;, 2013" />


    <title>word2vec - 魑魅魍魉</title>

        <link rel="stylesheet" href="../../../theme/css/bootstrap.min.css" type="text/css" />

    <link href="../../../theme/css/fontawesome-all.min.css" rel="stylesheet" />
    <link href="../../../theme/css/pygments/native.css" rel="stylesheet" />

    <link href="../../../theme/css/pelican-twitchy.min.css" rel="stylesheet" />

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->
    
    <!-- Feeds -->
</head>
<body data-spy="scroll" data-target="#scrollspy">
    <div id="wrapper">
        <!-- Sidebar -->
        <div id="sidebar-wrapper-small" class="twitchy-background">
            <ul id="accordion-small" class="sidebar-nav sidebar-nav-small">
                <li>
        <a href="../../.." title="魑魅魍魉" class="collapsed">
            <span class="fas fa-home"></span>
        </a>
    </li>
                <li class="nav-divider"></li>
                <li>
        <a href="../../../archives.html" title="Recent Articles" class="collapsed">
            <span class="fas fa-th-list"></span>
        </a>
    </li>
                <li class="nav-divider"></li>
                <li>
                    <a data-toggle="collapse" data-parent="#accordion-small" href="#collapse-social-small" title="Social" class="collapsed">
                        <i class="fas fa-users padding-small"></i>
                    </a>
                </li>
                <li class="panel anti-panel"><ul id="collapse-social-small" class="collapse ">
                    <li>
                        <a href="#" title="Another social link"><i class="fab fa-another-social-link-square padding-small"></i></a>
                    </li>
                    <li>
                        <a href="#" title="You can add links in your config file"><i class="fab fa-you-can-add-links-in-your-config-file-square padding-small"></i></a>
                    </li>
                </ul></li>
                <li class="nav-divider"></li>
                <li>
        <a href="#" title="Back to top" class="collapsed">
            <span class="fas fa-arrow-up"></span>
        </a>
    </li>
            </ul>
        </div>
        <div id="sidebar-wrapper" class="twitchy-background">
            <ul id="accordion" class="sidebar-nav">
                <li class="sidebar-brand">
                    <a href="../../../">
                            <span class="fas fa-home padding-small"></span>
                            魑魅魍魉
                    </a>
                </li>
                    <li>
                        <a href="../../../archives.html">
                            <span class="fas fa-th-list padding-small"></span>
                            Recent Articles
                        </a>
                    </li>
                <li class="nav-divider"></li>
                <li>
                    <a data-toggle="collapse" data-parent="#accordion" href="#collapse-social">
                        <i class="fas fa-users padding-small"></i>
                        Contact
                    </a>
                </li>
                <li class="panel anti-panel"><ul id="collapse-social" class="sidebar_submenu collapse ">
                    <li>
                        <a href="#" title="Another social link">
                            <i class="fab fa-another-social-link-square padding-small"></i>
                            Another social link
                        </a>
                    </li>
                    <li>
                        <a href="#" title="You can add links in your config file">
                            <i class="fab fa-you-can-add-links-in-your-config-file-square padding-small"></i>
                            You can add links in your config file
                        </a>
                    </li>
                </ul></li>
                
                <li class="nav-divider"></li>
                <li>
                    <a data-toggle="collapse" data-parent="#accordion" href="#collapse-pages">
                        <i class="fas fa-folder-open padding-small"></i>
                        Pages
                    </a>
                </li>
                <li class="panel anti-panel"><ul id="collapse-pages" class="sidebar_submenu collapse ">
                </ul></li>
                <li class="nav-divider"></li>
                <li>
                    <a data-toggle="collapse" data-parent="#accordion" href="#collapse-categories">
                        <i class="fas fa-folder-open padding-small"></i>
                        Categories
                    </a>
                </li>
                <li class="panel anti-panel"><ul id="collapse-categories" class="sidebar_submenu collapse ">
                    <li >
                        <a href="../../../category/00qi-ta-2.html">
                            <i class="fas fa-folder-open padding-small"></i>
                            00.其他2
                            <span class="badge badge-secondary float-right categorybadge">2</span>
                        </a>
                    </li>
                    <li >
                        <a href="../../../category/01chang-yong-gong-ju.html">
                            <i class="fas fa-folder-open padding-small"></i>
                            01常用工具
                            <span class="badge badge-secondary float-right categorybadge">10</span>
                        </a>
                    </li>
                    <li >
                        <a href="../../../category/02gong-ju-shi-yong.html">
                            <i class="fas fa-folder-open padding-small"></i>
                            02.工具使用
                            <span class="badge badge-secondary float-right categorybadge">2</span>
                        </a>
                    </li>
                    <li >
                        <a href="../../../category/02wo-ai-du-shu.html">
                            <i class="fas fa-folder-open padding-small"></i>
                            02.我爱读书
                            <span class="badge badge-secondary float-right categorybadge">1</span>
                        </a>
                    </li>
                    <li >
                        <a href="../../../category/03chang-yong-ming-ling.html">
                            <i class="fas fa-folder-open padding-small"></i>
                            03.常用命令
                            <span class="badge badge-secondary float-right categorybadge">3</span>
                        </a>
                    </li>
                    <li class="active">
                        <a href="../../../category/03shen-du-xue-xi.html">
                            <i class="fas fa-folder-open padding-small"></i>
                            03深度学习
                            <span class="badge badge-secondary float-right categorybadge">12</span>
                        </a>
                    </li>
                    <li >
                        <a href="../../../category/ji-chu-zhi-shi.html">
                            <i class="fas fa-folder-open padding-small"></i>
                            基础知识
                            <span class="badge badge-secondary float-right categorybadge">3</span>
                        </a>
                    </li>
                    <li >
                        <a href="../../../category/ji-qi-xue-xi-shi-zhan.html">
                            <i class="fas fa-folder-open padding-small"></i>
                            机器学习实战
                            <span class="badge badge-secondary float-right categorybadge">18</span>
                        </a>
                    </li>
                    <li >
                        <a href="../../../category/shen-du-xue-xi.html">
                            <i class="fas fa-folder-open padding-small"></i>
                            深度学习
                            <span class="badge badge-secondary float-right categorybadge">1</span>
                        </a>
                    </li>
                </ul></li>
            </ul>
        </div>
        <!-- /#sidebar-wrapper -->
        <!-- open/close sidebar -->
        <a href="#menu-toggle" class="btn btn-primary" id="menu-toggle">
            <span id="right-arrow" class="fas fa-chevron-right"  title="expand sidebar"></span>
            <span id="left-arrow" class="fas fa-chevron-left" title="minimize sidebar"></span>
        </a>
       <!-- /open/close sidebar -->

        <!-- Page Content -->
        <div id="page-content-wrapper">
            <div class="container-fluid">
<section id="content">
    <article>
        <div class="row">
            <div class="col-lg-9">
                <header class="page-header">
                    <h1>
                        <a href="../../../pages/2018/01/word2vec.html"
                           rel="bookmark"
                           title="Permalink to word2vec">
                            word2vec
                        </a>
                        <small>
<div class="post-info">
    <div class="publish-info-block">
        <small>
            <span class="published">
                <i class="fa fa-calendar padding-small"></i><time datetime="2018-01-01T00:00:00+08:00"> 2018-01-01 00:00</time>
            </span>
            <span class="category">
                <i class="fa fa-folder-open padding-small"></i><a href="../../../category/03shen-du-xue-xi.html">03深度学习</a>
            </span>
            <span class="tags">
                <i class="fa fa-tags padding-small"></i>
                <a href="../../../tag/tensorflowexample.html">TensorFlowExample</a>            </span>
        </small>
    </div>
</div><!-- /.post-info -->                        </small>
                    </h1>
                </header>
            </div>
        </div>
        <div class="row">
            <div class="col-lg-9">
                <div class="entry-content">
                    <style type="text/css">/*!
*
* IPython notebook
*
*/
/* CSS font colors for translated ANSI colors. */
.ansibold {
  font-weight: bold;
}
/* use dark versions for foreground, to improve visibility */
.ansiblack {
  color: black;
}
.ansired {
  color: darkred;
}
.ansigreen {
  color: darkgreen;
}
.ansiyellow {
  color: #c4a000;
}
.ansiblue {
  color: darkblue;
}
.ansipurple {
  color: darkviolet;
}
.ansicyan {
  color: steelblue;
}
.ansigray {
  color: gray;
}
/* and light for background, for the same reason */
.ansibgblack {
  background-color: black;
}
.ansibgred {
  background-color: red;
}
.ansibggreen {
  background-color: green;
}
.ansibgyellow {
  background-color: yellow;
}
.ansibgblue {
  background-color: blue;
}
.ansibgpurple {
  background-color: magenta;
}
.ansibgcyan {
  background-color: cyan;
}
.ansibggray {
  background-color: gray;
}
div.cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  border-radius: 2px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  border-width: 1px;
  border-style: solid;
  border-color: transparent;
  width: 100%;
  padding: 5px;
  /* This acts as a spacer between cells, that is outside the border */
  margin: 0px;
  outline: none;
  border-left-width: 1px;
  padding-left: 5px;
  background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%);
}
div.cell.jupyter-soft-selected {
  border-left-color: #90CAF9;
  border-left-color: #E3F2FD;
  border-left-width: 1px;
  padding-left: 5px;
  border-right-color: #E3F2FD;
  border-right-width: 1px;
  background: #E3F2FD;
}
@media print {
  div.cell.jupyter-soft-selected {
    border-color: transparent;
  }
}
div.cell.selected {
  border-color: #ababab;
  border-left-width: 0px;
  padding-left: 6px;
  background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%);
}
@media print {
  div.cell.selected {
    border-color: transparent;
  }
}
div.cell.selected.jupyter-soft-selected {
  border-left-width: 0;
  padding-left: 6px;
  background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%);
}
.edit_mode div.cell.selected {
  border-color: #66BB6A;
  border-left-width: 0px;
  padding-left: 6px;
  background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%);
}
@media print {
  .edit_mode div.cell.selected {
    border-color: transparent;
  }
}
.prompt {
  /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */
  min-width: 14ex;
  /* This padding is tuned to match the padding on the CodeMirror editor. */
  padding: 0.4em;
  margin: 0px;
  font-family: monospace;
  text-align: right;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
  /* Don't highlight prompt number selection */
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
  /* Use default cursor */
  cursor: default;
}
@media (max-width: 540px) {
  .prompt {
    text-align: left;
  }
}
div.inner_cell {
  min-width: 0;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_area {
  border: 1px solid #cfcfcf;
  border-radius: 2px;
  background: #f7f7f7;
  line-height: 1.21429em;
}
/* This is needed so that empty prompt areas can collapse to zero height when there
   is no content in the output_subarea and the prompt. The main purpose of this is
   to make sure that empty JavaScript output_subareas have no height. */
div.prompt:empty {
  padding-top: 0;
  padding-bottom: 0;
}
div.unrecognized_cell {
  padding: 5px 5px 5px 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.unrecognized_cell .inner_cell {
  border-radius: 2px;
  padding: 5px;
  font-weight: bold;
  color: red;
  border: 1px solid #cfcfcf;
  background: #eaeaea;
}
div.unrecognized_cell .inner_cell a {
  color: inherit;
  text-decoration: none;
}
div.unrecognized_cell .inner_cell a:hover {
  color: inherit;
  text-decoration: none;
}
@media (max-width: 540px) {
  div.unrecognized_cell > div.prompt {
    display: none;
  }
}
div.code_cell {
  /* avoid page breaking on code cells when printing */
}
@media print {
  div.code_cell {
    page-break-inside: avoid;
  }
}
/* any special styling for code cells that are currently running goes here */
div.input {
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.input {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_prompt {
  color: #303F9F;
  border-top: 1px solid transparent;
}
div.input_area > div.highlight {
  margin: 0.4em;
  border: none;
  padding: 0px;
  background-color: transparent;
}
div.input_area > div.highlight > pre {
  margin: 0px;
  border: none;
  padding: 0px;
  background-color: transparent;
}
/* The following gets added to the <head> if it is detected that the user has a
 * monospace font with inconsistent normal/bold/italic height.  See
 * notebookmain.js.  Such fonts will have keywords vertically offset with
 * respect to the rest of the text.  The user should select a better font.
 * See: https://github.com/ipython/ipython/issues/1503
 *
 * .CodeMirror span {
 *      vertical-align: bottom;
 * }
 */
.CodeMirror {
  line-height: 1.21429em;
  /* Changed from 1em to our global default */
  font-size: 14px;
  height: auto;
  /* Changed to auto to autogrow */
  background: none;
  /* Changed from white to allow our bg to show through */
}
.CodeMirror-scroll {
  /*  The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/
  /*  We have found that if it is visible, vertical scrollbars appear with font size changes.*/
  overflow-y: hidden;
  overflow-x: auto;
}
.CodeMirror-lines {
  /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */
  /* we have set a different line-height and want this to scale with that. */
  padding: 0.4em;
}
.CodeMirror-linenumber {
  padding: 0 8px 0 4px;
}
.CodeMirror-gutters {
  border-bottom-left-radius: 2px;
  border-top-left-radius: 2px;
}
.CodeMirror pre {
  /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */
  /* .CodeMirror-lines */
  padding: 0;
  border: 0;
  border-radius: 0;
}
/*

Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org>
Adapted from GitHub theme

*/
.highlight-base {
  color: #000;
}
.highlight-variable {
  color: #000;
}
.highlight-variable-2 {
  color: #1a1a1a;
}
.highlight-variable-3 {
  color: #333333;
}
.highlight-string {
  color: #BA2121;
}
.highlight-comment {
  color: #408080;
  font-style: italic;
}
.highlight-number {
  color: #080;
}
.highlight-atom {
  color: #88F;
}
.highlight-keyword {
  color: #008000;
  font-weight: bold;
}
.highlight-builtin {
  color: #008000;
}
.highlight-error {
  color: #f00;
}
.highlight-operator {
  color: #AA22FF;
  font-weight: bold;
}
.highlight-meta {
  color: #AA22FF;
}
/* previously not defined, copying from default codemirror */
.highlight-def {
  color: #00f;
}
.highlight-string-2 {
  color: #f50;
}
.highlight-qualifier {
  color: #555;
}
.highlight-bracket {
  color: #997;
}
.highlight-tag {
  color: #170;
}
.highlight-attribute {
  color: #00c;
}
.highlight-header {
  color: blue;
}
.highlight-quote {
  color: #090;
}
.highlight-link {
  color: #00c;
}
/* apply the same style to codemirror */
.cm-s-ipython span.cm-keyword {
  color: #008000;
  font-weight: bold;
}
.cm-s-ipython span.cm-atom {
  color: #88F;
}
.cm-s-ipython span.cm-number {
  color: #080;
}
.cm-s-ipython span.cm-def {
  color: #00f;
}
.cm-s-ipython span.cm-variable {
  color: #000;
}
.cm-s-ipython span.cm-operator {
  color: #AA22FF;
  font-weight: bold;
}
.cm-s-ipython span.cm-variable-2 {
  color: #1a1a1a;
}
.cm-s-ipython span.cm-variable-3 {
  color: #333333;
}
.cm-s-ipython span.cm-comment {
  color: #408080;
  font-style: italic;
}
.cm-s-ipython span.cm-string {
  color: #BA2121;
}
.cm-s-ipython span.cm-string-2 {
  color: #f50;
}
.cm-s-ipython span.cm-meta {
  color: #AA22FF;
}
.cm-s-ipython span.cm-qualifier {
  color: #555;
}
.cm-s-ipython span.cm-builtin {
  color: #008000;
}
.cm-s-ipython span.cm-bracket {
  color: #997;
}
.cm-s-ipython span.cm-tag {
  color: #170;
}
.cm-s-ipython span.cm-attribute {
  color: #00c;
}
.cm-s-ipython span.cm-header {
  color: blue;
}
.cm-s-ipython span.cm-quote {
  color: #090;
}
.cm-s-ipython span.cm-link {
  color: #00c;
}
.cm-s-ipython span.cm-error {
  color: #f00;
}
.cm-s-ipython span.cm-tab {
  background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=);
  background-position: right;
  background-repeat: no-repeat;
}
div.output_wrapper {
  /* this position must be relative to enable descendents to be absolute within it */
  position: relative;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  z-index: 1;
}
/* class for the output area when it should be height-limited */
div.output_scroll {
  /* ideally, this would be max-height, but FF barfs all over that */
  height: 24em;
  /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */
  width: 100%;
  overflow: auto;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  display: block;
}
/* output div while it is collapsed */
div.output_collapsed {
  margin: 0px;
  padding: 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
div.out_prompt_overlay {
  height: 100%;
  padding: 0px 0.4em;
  position: absolute;
  border-radius: 2px;
}
div.out_prompt_overlay:hover {
  /* use inner shadow to get border that is computed the same on WebKit/FF */
  -webkit-box-shadow: inset 0 0 1px #000;
  box-shadow: inset 0 0 1px #000;
  background: rgba(240, 240, 240, 0.5);
}
div.output_prompt {
  color: #D84315;
}
/* This class is the outer container of all output sections. */
div.output_area {
  padding: 0px;
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.output_area .MathJax_Display {
  text-align: left !important;
}
div.output_area 
div.output_area 
div.output_area img,
div.output_area svg {
  max-width: 100%;
  height: auto;
}
div.output_area img.unconfined,
div.output_area svg.unconfined {
  max-width: none;
}
/* This is needed to protect the pre formating from global settings such
   as that of bootstrap */
.output {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.output_area {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
div.output_area pre {
  margin: 0;
  padding: 0;
  border: 0;
  vertical-align: baseline;
  color: black;
  background-color: transparent;
  border-radius: 0;
}
/* This class is for the output subarea inside the output_area and after
   the prompt div. */
div.output_subarea {
  overflow-x: auto;
  padding: 0.4em;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
  max-width: calc(100% - 14ex);
}
div.output_scroll div.output_subarea {
  overflow-x: visible;
}
/* The rest of the output_* classes are for special styling of the different
   output types */
/* all text output has this class: */
div.output_text {
  text-align: left;
  color: #000;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
}
/* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */
div.output_stderr {
  background: #fdd;
  /* very light red background for stderr */
}
div.output_latex {
  text-align: left;
}
/* Empty output_javascript divs should have no height */
div.output_javascript:empty {
  padding: 0;
}
.js-error {
  color: darkred;
}
/* raw_input styles */
div.raw_input_container {
  line-height: 1.21429em;
  padding-top: 5px;
}
pre.raw_input_prompt {
  /* nothing needed here. */
}
input.raw_input {
  font-family: monospace;
  font-size: inherit;
  color: inherit;
  width: auto;
  /* make sure input baseline aligns with prompt */
  vertical-align: baseline;
  /* padding + margin = 0.5em between prompt and cursor */
  padding: 0em 0.25em;
  margin: 0em 0.25em;
}
input.raw_input:focus {
  box-shadow: none;
}
p.p-space {
  margin-bottom: 10px;
}
div.output_unrecognized {
  padding: 5px;
  font-weight: bold;
  color: red;
}
div.output_unrecognized a {
  color: inherit;
  text-decoration: none;
}
div.output_unrecognized a:hover {
  color: inherit;
  text-decoration: none;
}
.rendered_html {
  color: #000;
  /* any extras will just be numbers: */
}



.rendered_html :link {
  text-decoration: underline;
}
.rendered_html :visited {
  text-decoration: underline;
}






.rendered_html h1:first-child {
  margin-top: 0.538em;
}
.rendered_html h2:first-child {
  margin-top: 0.636em;
}
.rendered_html h3:first-child {
  margin-top: 0.777em;
}
.rendered_html h4:first-child {
  margin-top: 1em;
}
.rendered_html h5:first-child {
  margin-top: 1em;
}
.rendered_html h6:first-child {
  margin-top: 1em;
}








.rendered_html * + ul {
  margin-top: 1em;
}
.rendered_html * + ol {
  margin-top: 1em;
}


.rendered_html pre,



.rendered_html tr,
.rendered_html th,

.rendered_html td,


.rendered_html * + table {
  margin-top: 1em;
}

.rendered_html * + p {
  margin-top: 1em;
}

.rendered_html * + img {
  margin-top: 1em;
}
.rendered_html img,

.rendered_html img.unconfined,

div.text_cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.text_cell > div.prompt {
    display: none;
  }
}
div.text_cell_render {
  /*font-family: "Helvetica Neue", Arial, Helvetica, Geneva, sans-serif;*/
  outline: none;
  resize: none;
  width: inherit;
  border-style: none;
  padding: 0.5em 0.5em 0.5em 0.4em;
  color: #000;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
a.anchor-link:link {
  text-decoration: none;
  padding: 0px 20px;
  visibility: hidden;
}
h1:hover .anchor-link,
h2:hover .anchor-link,
h3:hover .anchor-link,
h4:hover .anchor-link,
h5:hover .anchor-link,
h6:hover .anchor-link {
  visibility: visible;
}
.text_cell.rendered .input_area {
  display: none;
}
.text_cell.rendered 
.text_cell.unrendered .text_cell_render {
  display: none;
}
.cm-header-1,
.cm-header-2,
.cm-header-3,
.cm-header-4,
.cm-header-5,
.cm-header-6 {
  font-weight: bold;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
}
.cm-header-1 {
  font-size: 185.7%;
}
.cm-header-2 {
  font-size: 157.1%;
}
.cm-header-3 {
  font-size: 128.6%;
}
.cm-header-4 {
  font-size: 110%;
}
.cm-header-5 {
  font-size: 100%;
  font-style: italic;
}
.cm-header-6 {
  font-size: 100%;
  font-style: italic;
}
</style>
<style type="text/css">.highlight .hll { background-color: #ffffcc }
.highlight  { background: #f8f8f8; }
.highlight .c { color: #408080; font-style: italic } /* Comment */
.highlight .err { border: 1px solid #FF0000 } /* Error */
.highlight .k { color: #008000; font-weight: bold } /* Keyword */
.highlight .o { color: #666666 } /* Operator */
.highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */
.highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */
.highlight .cp { color: #BC7A00 } /* Comment.Preproc */
.highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */
.highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */
.highlight .cs { color: #408080; font-style: italic } /* Comment.Special */
.highlight .gd { color: #A00000 } /* Generic.Deleted */
.highlight .ge { font-style: italic } /* Generic.Emph */
.highlight .gr { color: #FF0000 } /* Generic.Error */
.highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */
.highlight .gi { color: #00A000 } /* Generic.Inserted */
.highlight .go { color: #888888 } /* Generic.Output */
.highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
.highlight .gs { font-weight: bold } /* Generic.Strong */
.highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
.highlight .gt { color: #0044DD } /* Generic.Traceback */
.highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: #008000 } /* Keyword.Pseudo */
.highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: #B00040 } /* Keyword.Type */
.highlight .m { color: #666666 } /* Literal.Number */
.highlight .s { color: #BA2121 } /* Literal.String */
.highlight .na { color: #7D9029 } /* Name.Attribute */
.highlight .nb { color: #008000 } /* Name.Builtin */
.highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */
.highlight .no { color: #880000 } /* Name.Constant */
.highlight .nd { color: #AA22FF } /* Name.Decorator */
.highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */
.highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */
.highlight .nf { color: #0000FF } /* Name.Function */
.highlight .nl { color: #A0A000 } /* Name.Label */
.highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */
.highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */
.highlight .nv { color: #19177C } /* Name.Variable */
.highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */
.highlight .w { color: #bbbbbb } /* Text.Whitespace */
.highlight .mb { color: #666666 } /* Literal.Number.Bin */
.highlight .mf { color: #666666 } /* Literal.Number.Float */
.highlight .mh { color: #666666 } /* Literal.Number.Hex */
.highlight .mi { color: #666666 } /* Literal.Number.Integer */
.highlight .mo { color: #666666 } /* Literal.Number.Oct */
.highlight .sa { color: #BA2121 } /* Literal.String.Affix */
.highlight .sb { color: #BA2121 } /* Literal.String.Backtick */
.highlight .sc { color: #BA2121 } /* Literal.String.Char */
.highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */
.highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
.highlight .s2 { color: #BA2121 } /* Literal.String.Double */
.highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */
.highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */
.highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */
.highlight .sx { color: #008000 } /* Literal.String.Other */
.highlight .sr { color: #BB6688 } /* Literal.String.Regex */
.highlight .s1 { color: #BA2121 } /* Literal.String.Single */
.highlight .ss { color: #19177C } /* Literal.String.Symbol */
.highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */
.highlight .fm { color: #0000FF } /* Name.Function.Magic */
.highlight .vc { color: #19177C } /* Name.Variable.Class */
.highlight .vg { color: #19177C } /* Name.Variable.Global */
.highlight .vi { color: #19177C } /* Name.Variable.Instance */
.highlight .vm { color: #19177C } /* Name.Variable.Magic */
.highlight .il { color: #666666 } /* Literal.Number.Integer.Long */</style>
<style type="text/css">
/* Temporary definitions which will become obsolete with Notebook release 5.0 */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-bold { font-weight: bold; }
</style><body>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Word2Vec-(Word-Embedding)">Word2Vec (Word Embedding)<a class="anchor-link" href="#Word2Vec-(Word-Embedding)">¶</a></h1><p>Implement Word2Vec algorithm to compute vector representations of words.
This example is using a small chunk of Wikipedia articles to train from.</p>
<p>More info: <a href="https://arxiv.org/pdf/1301.3781.pdf">Mikolov, Tomas et al. "Efficient Estimation of Word Representations in Vector Space.", 2013</a></p>
<ul>
<li>Author: Aymeric Damien</li>
<li>Project: <a href="https://github.com/aymericdamien/TensorFlow-Examples/">https://github.com/aymericdamien/TensorFlow-Examples/</a></li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [1]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">division</span><span class="p">,</span> <span class="n">print_function</span><span class="p">,</span> <span class="n">absolute_import</span>

<span class="kn">import</span> <span class="nn">collections</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">urllib</span>
<span class="kn">import</span> <span class="nn">zipfile</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [2]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Training Parameters</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">num_steps</span> <span class="o">=</span> <span class="mi">3000000</span>
<span class="n">display_step</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">eval_step</span> <span class="o">=</span> <span class="mi">200000</span>

<span class="c1"># Evaluation Parameters</span>
<span class="n">eval_words</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'five'</span><span class="p">,</span> <span class="s1">'of'</span><span class="p">,</span> <span class="s1">'going'</span><span class="p">,</span> <span class="s1">'hardware'</span><span class="p">,</span> <span class="s1">'american'</span><span class="p">,</span> <span class="s1">'britain'</span><span class="p">]</span>

<span class="c1"># Word2Vec Parameters</span>
<span class="n">embedding_size</span> <span class="o">=</span> <span class="mi">200</span> <span class="c1"># Dimension of the embedding vector</span>
<span class="n">max_vocabulary_size</span> <span class="o">=</span> <span class="mi">50000</span> <span class="c1"># Total number of different words in the vocabulary</span>
<span class="n">min_occurrence</span> <span class="o">=</span> <span class="mi">10</span> <span class="c1"># Remove all words that does not appears at least n times</span>
<span class="n">skip_window</span> <span class="o">=</span> <span class="mi">3</span> <span class="c1"># How many words to consider left and right</span>
<span class="n">num_skips</span> <span class="o">=</span> <span class="mi">2</span> <span class="c1"># How many times to reuse an input to generate a label</span>
<span class="n">num_sampled</span> <span class="o">=</span> <span class="mi">64</span> <span class="c1"># Number of negative examples to sample</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [6]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Download a small chunk of Wikipedia articles collection</span>
<span class="n">url</span> <span class="o">=</span> <span class="s1">'http://mattmahoney.net/dc/text8.zip'</span>
<span class="n">data_path</span> <span class="o">=</span> <span class="s1">'text8.zip'</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">data_path</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Downloading the dataset... (It may take some time)"</span><span class="p">)</span>
    <span class="n">filename</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">urllib</span><span class="o">.</span><span class="n">urlretrieve</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">data_path</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Done!"</span><span class="p">)</span>
<span class="c1"># Unzip the dataset file. Text has already been processed</span>
<span class="k">with</span> <span class="n">zipfile</span><span class="o">.</span><span class="n">ZipFile</span><span class="p">(</span><span class="n">data_path</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">text_words</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">namelist</span><span class="p">()[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [4]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Build the dictionary and replace rare words with UNK token</span>
<span class="n">count</span> <span class="o">=</span> <span class="p">[(</span><span class="s1">'UNK'</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)]</span>
<span class="c1"># Retrieve the most common words</span>
<span class="n">count</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">collections</span><span class="o">.</span><span class="n">Counter</span><span class="p">(</span><span class="n">text_words</span><span class="p">)</span><span class="o">.</span><span class="n">most_common</span><span class="p">(</span><span class="n">max_vocabulary_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
<span class="c1"># Remove samples with less than 'min_occurrence' occurrences</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">count</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">count</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">min_occurrence</span><span class="p">:</span>
        <span class="n">count</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># The collection is ordered, so stop when 'min_occurrence' is reached</span>
        <span class="k">break</span>
<span class="c1"># Compute the vocabulary size</span>
<span class="n">vocabulary_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">count</span><span class="p">)</span>
<span class="c1"># Assign an id to each word</span>
<span class="n">word2id</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span><span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">count</span><span class="p">):</span>
    <span class="n">word2id</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="n">i</span>

<span class="n">data</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
<span class="n">unk_count</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">text_words</span><span class="p">:</span>
    <span class="c1"># Retrieve a word id, or assign it index 0 ('UNK') if not in dictionary</span>
    <span class="n">index</span> <span class="o">=</span> <span class="n">word2id</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">index</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">unk_count</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">data</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
<span class="n">count</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="s1">'UNK'</span><span class="p">,</span> <span class="n">unk_count</span><span class="p">)</span>
<span class="n">id2word</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">word2id</span><span class="o">.</span><span class="n">values</span><span class="p">(),</span> <span class="n">word2id</span><span class="o">.</span><span class="n">keys</span><span class="p">()))</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"Words count:"</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">text_words</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Unique words:"</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">text_words</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Vocabulary size:"</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Most common words:"</span><span class="p">,</span> <span class="n">count</span><span class="p">[:</span><span class="mi">10</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>Words count: 17005207
Unique words: 253854
Vocabulary size: 50000
Most common words: [('UNK', 418391), ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764), ('in', 372201), ('a', 325873), ('to', 316376), ('zero', 264975), ('nine', 250430)]
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [5]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">data_index</span> <span class="o">=</span> <span class="mi">0</span>
<span class="c1"># Generate training batch for the skip-gram model</span>
<span class="k">def</span> <span class="nf">next_batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_skips</span><span class="p">,</span> <span class="n">skip_window</span><span class="p">):</span>
    <span class="k">global</span> <span class="n">data_index</span>
    <span class="k">assert</span> <span class="n">batch_size</span> <span class="o">%</span> <span class="n">num_skips</span> <span class="o">==</span> <span class="mi">0</span>
    <span class="k">assert</span> <span class="n">num_skips</span> <span class="o">&lt;=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">skip_window</span>
    <span class="n">batch</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">batch_size</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="c1"># get window size (words left and right + current one)</span>
    <span class="n">span</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">skip_window</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">buffer</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">span</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">data_index</span> <span class="o">+</span> <span class="n">span</span> <span class="o">&gt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
        <span class="n">data_index</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">buffer</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">data_index</span><span class="p">:</span><span class="n">data_index</span> <span class="o">+</span> <span class="n">span</span><span class="p">])</span>
    <span class="n">data_index</span> <span class="o">+=</span> <span class="n">span</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span> <span class="o">//</span> <span class="n">num_skips</span><span class="p">):</span>
        <span class="n">context_words</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">span</span><span class="p">)</span> <span class="k">if</span> <span class="n">w</span> <span class="o">!=</span> <span class="n">skip_window</span><span class="p">]</span>
        <span class="n">words_to_use</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">context_words</span><span class="p">,</span> <span class="n">num_skips</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">context_word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">words_to_use</span><span class="p">):</span>
            <span class="n">batch</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">num_skips</span> <span class="o">+</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">buffer</span><span class="p">[</span><span class="n">skip_window</span><span class="p">]</span>
            <span class="n">labels</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">num_skips</span> <span class="o">+</span> <span class="n">j</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">buffer</span><span class="p">[</span><span class="n">context_word</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">data_index</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
            <span class="n">buffer</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">span</span><span class="p">])</span>
            <span class="n">data_index</span> <span class="o">=</span> <span class="n">span</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">buffer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">data_index</span><span class="p">])</span>
            <span class="n">data_index</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="c1"># Backtrack a little bit to avoid skipping words in the end of a batch</span>
    <span class="n">data_index</span> <span class="o">=</span> <span class="p">(</span><span class="n">data_index</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span> <span class="o">-</span> <span class="n">span</span><span class="p">)</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">batch</span><span class="p">,</span> <span class="n">labels</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [6]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Input data</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="kc">None</span><span class="p">])</span>
<span class="c1"># Input label</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

<span class="c1"># Ensure the following ops &amp; var are assigned on CPU</span>
<span class="c1"># (some ops are not compatible on GPU)</span>
<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">'/cpu:0'</span><span class="p">):</span>
    <span class="c1"># Create the embedding variable (each row represent a word embedding vector)</span>
    <span class="n">embedding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_normal</span><span class="p">([</span><span class="n">vocabulary_size</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">]))</span>
    <span class="c1"># Lookup the corresponding embedding vectors for each sample in X</span>
    <span class="n">X_embed</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">embedding_lookup</span><span class="p">(</span><span class="n">embedding</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>

    <span class="c1"># Construct the variables for the NCE loss</span>
    <span class="n">nce_weights</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_normal</span><span class="p">([</span><span class="n">vocabulary_size</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">]))</span>
    <span class="n">nce_biases</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">vocabulary_size</span><span class="p">]))</span>

<span class="c1"># Compute the average NCE loss for the batch</span>
<span class="n">loss_op</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">nce_loss</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="n">nce_weights</span><span class="p">,</span>
                   <span class="n">biases</span><span class="o">=</span><span class="n">nce_biases</span><span class="p">,</span>
                   <span class="n">labels</span><span class="o">=</span><span class="n">Y</span><span class="p">,</span>
                   <span class="n">inputs</span><span class="o">=</span><span class="n">X_embed</span><span class="p">,</span>
                   <span class="n">num_sampled</span><span class="o">=</span><span class="n">num_sampled</span><span class="p">,</span>
                   <span class="n">num_classes</span><span class="o">=</span><span class="n">vocabulary_size</span><span class="p">))</span>

<span class="c1"># Define the optimizer</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">GradientDescentOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">)</span>
<span class="n">train_op</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">loss_op</span><span class="p">)</span>

<span class="c1"># Evaluation</span>
<span class="c1"># Compute the cosine similarity between input data embedding and every embedding vectors</span>
<span class="n">X_embed_norm</span> <span class="o">=</span> <span class="n">X_embed</span> <span class="o">/</span> <span class="n">tf</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">X_embed</span><span class="p">)))</span>
<span class="n">embedding_norm</span> <span class="o">=</span> <span class="n">embedding</span> <span class="o">/</span> <span class="n">tf</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">embedding</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="n">cosine_sim_op</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">X_embed_norm</span><span class="p">,</span> <span class="n">embedding_norm</span><span class="p">,</span> <span class="n">transpose_b</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [7]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Initialize the variables (i.e. assign their default value)</span>
<span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>

    <span class="c1"># Run the initializer</span>
    <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">init</span><span class="p">)</span>

    <span class="c1"># Testing data</span>
    <span class="n">x_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">word2id</span><span class="p">[</span><span class="n">w</span><span class="p">]</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">eval_words</span><span class="p">])</span>

    <span class="n">average_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="n">xrange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_steps</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="c1"># Get a new batch of data</span>
        <span class="n">batch_x</span><span class="p">,</span> <span class="n">batch_y</span> <span class="o">=</span> <span class="n">next_batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_skips</span><span class="p">,</span> <span class="n">skip_window</span><span class="p">)</span>
        <span class="c1"># Run training op</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">train_op</span><span class="p">,</span> <span class="n">loss_op</span><span class="p">],</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">batch_x</span><span class="p">,</span> <span class="n">Y</span><span class="p">:</span> <span class="n">batch_y</span><span class="p">})</span>
        <span class="n">average_loss</span> <span class="o">+=</span> <span class="n">loss</span>

        <span class="k">if</span> <span class="n">step</span> <span class="o">%</span> <span class="n">display_step</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">step</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">step</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">average_loss</span> <span class="o">/=</span> <span class="n">display_step</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">"Step "</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">step</span><span class="p">)</span> <span class="o">+</span> <span class="s2">", Average Loss= "</span> <span class="o">+</span> \
                  <span class="s2">"</span><span class="si">{:.4f}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">average_loss</span><span class="p">))</span>
            <span class="n">average_loss</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c1"># Evaluation</span>
        <span class="k">if</span> <span class="n">step</span> <span class="o">%</span> <span class="n">eval_step</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">step</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">"Evaluation..."</span><span class="p">)</span>
            <span class="n">sim</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">cosine_sim_op</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">x_test</span><span class="p">})</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">xrange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">eval_words</span><span class="p">)):</span>
                <span class="n">top_k</span> <span class="o">=</span> <span class="mi">8</span>  <span class="c1"># number of nearest neighbors</span>
                <span class="n">nearest</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="n">sim</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:])</span><span class="o">.</span><span class="n">argsort</span><span class="p">()[</span><span class="mi">1</span><span class="p">:</span><span class="n">top_k</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>
                <span class="n">log_str</span> <span class="o">=</span> <span class="s1">'"</span><span class="si">%s</span><span class="s1">" nearest neighbors:'</span> <span class="o">%</span> <span class="n">eval_words</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
                <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">xrange</span><span class="p">(</span><span class="n">top_k</span><span class="p">):</span>
                    <span class="n">log_str</span> <span class="o">=</span> <span class="s1">'</span><span class="si">%s</span><span class="s1"> </span><span class="si">%s</span><span class="s1">,'</span> <span class="o">%</span> <span class="p">(</span><span class="n">log_str</span><span class="p">,</span> <span class="n">id2word</span><span class="p">[</span><span class="n">nearest</span><span class="p">[</span><span class="n">k</span><span class="p">]])</span>
                <span class="nb">print</span><span class="p">(</span><span class="n">log_str</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>Step 1, Average Loss= 520.3188
Evaluation...
"five" nearest neighbors: brothers, swinging, dissemination, fruitful, trichloride, dll, timur, torre,
"of" nearest neighbors: malting, vaginal, cecil, xiaoping, arrangers, hydras, exhibits, splits,
"going" nearest neighbors: besht, xps, sdtv, mississippi, frequencies, tora, reciprocating, tursiops,
"hardware" nearest neighbors: burgh, residences, mares, attested, whirlwind, isomerism, admiration, ties,
"american" nearest neighbors: tensile, months, baffling, cricket, kodak, risky, nicomedia, jura,
"britain" nearest neighbors: superstring, interpretations, genealogical, munition, boer, occasional, psychologists, turbofan,
Step 10000, Average Loss= 202.2640
Step 20000, Average Loss= 96.5149
Step 30000, Average Loss= 67.2858
Step 40000, Average Loss= 52.5055
Step 50000, Average Loss= 42.6301
Step 60000, Average Loss= 37.3644
Step 70000, Average Loss= 33.1220
Step 80000, Average Loss= 30.5835
Step 90000, Average Loss= 28.2243
Step 100000, Average Loss= 25.5532
Step 110000, Average Loss= 24.0891
Step 120000, Average Loss= 21.8576
Step 130000, Average Loss= 21.2192
Step 140000, Average Loss= 19.8834
Step 150000, Average Loss= 19.3362
Step 160000, Average Loss= 18.3129
Step 170000, Average Loss= 17.4952
Step 180000, Average Loss= 16.8531
Step 190000, Average Loss= 15.9615
Step 200000, Average Loss= 15.0718
Evaluation...
"five" nearest neighbors: three, four, eight, six, seven, two, nine, one,
"of" nearest neighbors: the, is, a, was, with, in, and, on,
"going" nearest neighbors: time, military, called, with, used, state, most, new,
"hardware" nearest neighbors: deaths, system, three, at, zero, two, s, UNK,
"american" nearest neighbors: UNK, and, s, about, in, when, from, after,
"britain" nearest neighbors: years, were, from, both, of, these, is, many,
Step 210000, Average Loss= 14.9267
Step 220000, Average Loss= 15.4700
Step 230000, Average Loss= 14.0867
Step 240000, Average Loss= 14.5337
Step 250000, Average Loss= 13.2458
Step 260000, Average Loss= 13.2944
Step 270000, Average Loss= 13.0396
Step 280000, Average Loss= 12.1902
Step 290000, Average Loss= 11.7444
Step 300000, Average Loss= 11.8473
Step 310000, Average Loss= 11.1306
Step 320000, Average Loss= 11.1699
Step 330000, Average Loss= 10.8638
Step 340000, Average Loss= 10.7910
Step 350000, Average Loss= 11.0721
Step 360000, Average Loss= 10.6309
Step 370000, Average Loss= 10.4836
Step 380000, Average Loss= 10.3482
Step 390000, Average Loss= 10.0679
Step 400000, Average Loss= 10.0070
Evaluation...
"five" nearest neighbors: four, three, six, seven, eight, two, one, zero,
"of" nearest neighbors: and, in, the, a, for, by, is, while,
"going" nearest neighbors: name, called, made, military, music, people, city, was,
"hardware" nearest neighbors: power, a, john, the, has, see, and, system,
"american" nearest neighbors: s, british, UNK, john, in, during, and, from,
"britain" nearest neighbors: from, general, are, before, first, after, history, was,
Step 410000, Average Loss= 10.1151
Step 420000, Average Loss= 9.5719
Step 430000, Average Loss= 9.8267
Step 440000, Average Loss= 9.4704
Step 450000, Average Loss= 9.5561
Step 460000, Average Loss= 9.1479
Step 470000, Average Loss= 8.8914
Step 480000, Average Loss= 9.0281
Step 490000, Average Loss= 9.3139
Step 500000, Average Loss= 9.1559
Step 510000, Average Loss= 8.8257
Step 520000, Average Loss= 8.9081
Step 530000, Average Loss= 8.8572
Step 540000, Average Loss= 8.5835
Step 550000, Average Loss= 8.4495
Step 560000, Average Loss= 8.4193
Step 570000, Average Loss= 8.3399
Step 580000, Average Loss= 8.1633
Step 590000, Average Loss= 8.2914
Step 600000, Average Loss= 8.0268
Evaluation...
"five" nearest neighbors: three, four, six, two, seven, eight, one, zero,
"of" nearest neighbors: and, the, in, including, with, for, on, or,
"going" nearest neighbors: popular, king, his, music, and, time, name, being,
"hardware" nearest neighbors: power, over, then, than, became, at, less, for,
"american" nearest neighbors: english, s, german, in, french, since, john, between,
"britain" nearest neighbors: however, were, state, first, group, general, from, second,
Step 610000, Average Loss= 8.1733
Step 620000, Average Loss= 8.2522
Step 630000, Average Loss= 8.0434
Step 640000, Average Loss= 8.0930
Step 650000, Average Loss= 7.8770
Step 660000, Average Loss= 7.9221
Step 670000, Average Loss= 7.7645
Step 680000, Average Loss= 7.9534
Step 690000, Average Loss= 7.7507
Step 700000, Average Loss= 7.7499
Step 710000, Average Loss= 7.6629
Step 720000, Average Loss= 7.6055
Step 730000, Average Loss= 7.4779
Step 740000, Average Loss= 7.3182
Step 750000, Average Loss= 7.6399
Step 760000, Average Loss= 7.4364
Step 770000, Average Loss= 7.6509
Step 780000, Average Loss= 7.3204
Step 790000, Average Loss= 7.4101
Step 800000, Average Loss= 7.4354
Evaluation...
"five" nearest neighbors: three, four, six, seven, eight, two, one, nine,
"of" nearest neighbors: and, the, its, a, with, at, in, for,
"going" nearest neighbors: were, man, music, now, great, support, popular, her,
"hardware" nearest neighbors: power, system, then, military, high, against, since, international,
"american" nearest neighbors: english, british, born, b, john, french, d, german,
"britain" nearest neighbors: government, second, before, from, state, several, the, at,
Step 810000, Average Loss= 7.2603
Step 820000, Average Loss= 7.1646
Step 830000, Average Loss= 7.3155
Step 840000, Average Loss= 7.1274
Step 850000, Average Loss= 7.1237
Step 860000, Average Loss= 7.1528
Step 870000, Average Loss= 7.0673
Step 880000, Average Loss= 7.2167
Step 890000, Average Loss= 7.1359
Step 900000, Average Loss= 7.0940
Step 910000, Average Loss= 7.1114
Step 920000, Average Loss= 6.9328
Step 930000, Average Loss= 7.0108
Step 940000, Average Loss= 7.0630
Step 950000, Average Loss= 6.8371
Step 960000, Average Loss= 7.0466
Step 970000, Average Loss= 6.8331
Step 980000, Average Loss= 6.9670
Step 990000, Average Loss= 6.7357
Step 1000000, Average Loss= 6.6453
Evaluation...
"five" nearest neighbors: four, three, six, eight, seven, two, nine, zero,
"of" nearest neighbors: the, became, including, first, second, from, following, and,
"going" nearest neighbors: near, music, popular, made, while, his, works, most,
"hardware" nearest neighbors: power, system, before, its, using, for, thus, an,
"american" nearest neighbors: b, born, d, UNK, nine, john, english, seven,
"britain" nearest neighbors: of, following, government, home, from, state, end, several,
Step 1010000, Average Loss= 6.7193
Step 1020000, Average Loss= 6.9297
Step 1030000, Average Loss= 6.7905
Step 1040000, Average Loss= 6.7709
Step 1050000, Average Loss= 6.7337
Step 1060000, Average Loss= 6.7617
Step 1070000, Average Loss= 6.7489
Step 1080000, Average Loss= 6.6259
Step 1090000, Average Loss= 6.6415
Step 1100000, Average Loss= 6.7209
Step 1110000, Average Loss= 6.5471
Step 1120000, Average Loss= 6.6508
Step 1130000, Average Loss= 6.5184
Step 1140000, Average Loss= 6.6202
Step 1150000, Average Loss= 6.7205
Step 1160000, Average Loss= 6.5821
Step 1170000, Average Loss= 6.6200
Step 1180000, Average Loss= 6.5089
Step 1190000, Average Loss= 6.5587
Step 1200000, Average Loss= 6.4930
Evaluation...
"five" nearest neighbors: three, four, six, seven, eight, two, nine, zero,
"of" nearest neighbors: the, and, including, in, first, with, following, from,
"going" nearest neighbors: near, popular, works, today, large, now, when, both,
"hardware" nearest neighbors: power, system, computer, its, both, for, using, which,
"american" nearest neighbors: born, d, john, german, b, UNK, english, s,
"britain" nearest neighbors: state, following, government, home, became, people, were, the,
Step 1210000, Average Loss= 6.5985
Step 1220000, Average Loss= 6.4534
Step 1230000, Average Loss= 6.5083
Step 1240000, Average Loss= 6.4913
Step 1250000, Average Loss= 6.4326
Step 1260000, Average Loss= 6.3891
Step 1270000, Average Loss= 6.1601
Step 1280000, Average Loss= 6.4479
Step 1290000, Average Loss= 6.3813
Step 1300000, Average Loss= 6.5335
Step 1310000, Average Loss= 6.2971
Step 1320000, Average Loss= 6.3723
Step 1330000, Average Loss= 6.4234
Step 1340000, Average Loss= 6.3130
Step 1350000, Average Loss= 6.2867
Step 1360000, Average Loss= 6.3505
Step 1370000, Average Loss= 6.2990
Step 1380000, Average Loss= 6.3012
Step 1390000, Average Loss= 6.3112
Step 1400000, Average Loss= 6.2680
Evaluation...
"five" nearest neighbors: four, three, six, two, seven, eight, one, zero,
"of" nearest neighbors: the, its, and, including, in, with, see, for,
"going" nearest neighbors: near, great, like, today, began, called, an, another,
"hardware" nearest neighbors: power, computer, system, for, program, high, control, small,
"american" nearest neighbors: english, german, french, born, john, british, s, references,
"britain" nearest neighbors: state, great, government, people, following, became, along, home,
Step 1410000, Average Loss= 6.3157
Step 1420000, Average Loss= 6.3466
Step 1430000, Average Loss= 6.3090
Step 1440000, Average Loss= 6.3330
Step 1450000, Average Loss= 6.2072
Step 1460000, Average Loss= 6.2363
Step 1470000, Average Loss= 6.2736
Step 1480000, Average Loss= 6.1793
Step 1490000, Average Loss= 6.2977
Step 1500000, Average Loss= 6.1899
Step 1510000, Average Loss= 6.2381
Step 1520000, Average Loss= 6.1027
Step 1530000, Average Loss= 6.0046
Step 1540000, Average Loss= 6.0747
Step 1550000, Average Loss= 6.2524
Step 1560000, Average Loss= 6.1247
Step 1570000, Average Loss= 6.1937
Step 1580000, Average Loss= 6.0450
Step 1590000, Average Loss= 6.1556
Step 1600000, Average Loss= 6.1765
Evaluation...
"five" nearest neighbors: three, four, six, two, seven, eight, one, zero,
"of" nearest neighbors: the, and, its, for, from, modern, in, part,
"going" nearest neighbors: great, today, once, now, while, her, like, by,
"hardware" nearest neighbors: power, system, high, program, control, computer, typically, making,
"american" nearest neighbors: born, english, british, german, john, french, b, d,
"britain" nearest neighbors: country, state, home, government, first, following, during, from,
Step 1610000, Average Loss= 6.1029
Step 1620000, Average Loss= 6.0501
Step 1630000, Average Loss= 6.1536
Step 1640000, Average Loss= 6.0483
Step 1650000, Average Loss= 6.1197
Step 1660000, Average Loss= 6.0261
Step 1670000, Average Loss= 6.1012
Step 1680000, Average Loss= 6.1795
Step 1690000, Average Loss= 6.1224
Step 1700000, Average Loss= 6.0896
Step 1710000, Average Loss= 6.0418
Step 1720000, Average Loss= 6.0626
Step 1730000, Average Loss= 6.0214
Step 1740000, Average Loss= 6.1206
Step 1750000, Average Loss= 5.9721
Step 1760000, Average Loss= 6.0782
Step 1770000, Average Loss= 6.0291
Step 1780000, Average Loss= 6.0187
Step 1790000, Average Loss= 5.9761
Step 1800000, Average Loss= 5.7518
Evaluation...
"five" nearest neighbors: four, three, six, seven, eight, nine, two, zero,
"of" nearest neighbors: the, from, in, became, and, second, first, including,
"going" nearest neighbors: today, which, once, little, made, before, now, etc,
"hardware" nearest neighbors: computer, power, program, system, high, typically, current, eventually,
"american" nearest neighbors: b, d, born, actor, UNK, robert, william, english,
"britain" nearest neighbors: government, state, country, from, world, great, of, in,
Step 1810000, Average Loss= 5.9839
Step 1820000, Average Loss= 5.9931
Step 1830000, Average Loss= 6.0794
Step 1840000, Average Loss= 5.9072
Step 1850000, Average Loss= 5.9831
Step 1860000, Average Loss= 6.0023
Step 1870000, Average Loss= 5.9375
Step 1880000, Average Loss= 5.9250
Step 1890000, Average Loss= 5.9422
Step 1900000, Average Loss= 5.9339
Step 1910000, Average Loss= 5.9235
Step 1920000, Average Loss= 5.9692
Step 1930000, Average Loss= 5.9022
Step 1940000, Average Loss= 5.9599
Step 1950000, Average Loss= 6.0174
Step 1960000, Average Loss= 5.9530
Step 1970000, Average Loss= 5.9479
Step 1980000, Average Loss= 5.8870
Step 1990000, Average Loss= 5.9271
Step 2000000, Average Loss= 5.8774
Evaluation...
"five" nearest neighbors: four, three, six, seven, eight, two, nine, zero,
"of" nearest neighbors: and, the, from, in, within, first, including, with,
"going" nearest neighbors: today, before, another, little, work, etc, now, him,
"hardware" nearest neighbors: computer, program, system, both, making, designed, power, simple,
"american" nearest neighbors: actor, born, d, robert, john, b, german, writer,
"britain" nearest neighbors: government, state, following, great, england, became, country, from,
Step 2010000, Average Loss= 5.9373
Step 2020000, Average Loss= 5.9113
Step 2030000, Average Loss= 5.9158
Step 2040000, Average Loss= 5.9020
Step 2050000, Average Loss= 5.8608
Step 2060000, Average Loss= 5.7379
Step 2070000, Average Loss= 5.7143
Step 2080000, Average Loss= 5.9379
Step 2090000, Average Loss= 5.8201
Step 2100000, Average Loss= 5.9390
Step 2110000, Average Loss= 5.7295
Step 2120000, Average Loss= 5.8290
Step 2130000, Average Loss= 5.9042
Step 2140000, Average Loss= 5.8367
Step 2150000, Average Loss= 5.7760
Step 2160000, Average Loss= 5.8664
Step 2170000, Average Loss= 5.7974
Step 2180000, Average Loss= 5.8523
Step 2190000, Average Loss= 5.8047
Step 2200000, Average Loss= 5.8172
Evaluation...
"five" nearest neighbors: three, four, six, eight, two, seven, one, zero,
"of" nearest neighbors: the, with, group, in, its, and, from, including,
"going" nearest neighbors: produced, when, today, while, little, before, had, like,
"hardware" nearest neighbors: computer, system, power, technology, program, simple, for, designed,
"american" nearest neighbors: english, canadian, german, french, author, british, film, born,
"britain" nearest neighbors: government, great, state, established, british, england, country, army,
Step 2210000, Average Loss= 5.8847
Step 2220000, Average Loss= 5.8622
Step 2230000, Average Loss= 5.8295
Step 2240000, Average Loss= 5.8484
Step 2250000, Average Loss= 5.7917
Step 2260000, Average Loss= 5.7846
Step 2270000, Average Loss= 5.8307
Step 2280000, Average Loss= 5.7341
Step 2290000, Average Loss= 5.8519
Step 2300000, Average Loss= 5.7792
Step 2310000, Average Loss= 5.8277
Step 2320000, Average Loss= 5.7196
Step 2330000, Average Loss= 5.5469
Step 2340000, Average Loss= 5.7177
Step 2350000, Average Loss= 5.8139
Step 2360000, Average Loss= 5.7849
Step 2370000, Average Loss= 5.7022
Step 2380000, Average Loss= 5.7447
Step 2390000, Average Loss= 5.7667
Step 2400000, Average Loss= 5.7625
Evaluation...
"five" nearest neighbors: three, four, six, seven, two, eight, zero, nine,
"of" nearest neighbors: the, and, from, part, in, following, within, including,
"going" nearest neighbors: where, once, little, now, again, while, off, produced,
"hardware" nearest neighbors: system, computer, high, power, using, designed, systems, simple,
"american" nearest neighbors: author, actor, english, born, writer, british, b, d,
"britain" nearest neighbors: great, established, government, england, country, state, army, former,
Step 2410000, Average Loss= 5.6953
Step 2420000, Average Loss= 5.7413
Step 2430000, Average Loss= 5.7242
Step 2440000, Average Loss= 5.7397
Step 2450000, Average Loss= 5.7755
Step 2460000, Average Loss= 5.6881
Step 2470000, Average Loss= 5.7471
Step 2480000, Average Loss= 5.8159
Step 2490000, Average Loss= 5.7452
Step 2500000, Average Loss= 5.7547
Step 2510000, Average Loss= 5.6945
Step 2520000, Average Loss= 5.7318
Step 2530000, Average Loss= 5.6682
Step 2540000, Average Loss= 5.7660
Step 2550000, Average Loss= 5.6956
Step 2560000, Average Loss= 5.7307
Step 2570000, Average Loss= 5.7015
Step 2580000, Average Loss= 5.6932
Step 2590000, Average Loss= 5.6386
Step 2600000, Average Loss= 5.4734
Evaluation...
"five" nearest neighbors: four, three, six, seven, eight, nine, two, zero,
"of" nearest neighbors: the, and, in, from, became, including, for, with,
"going" nearest neighbors: little, again, just, a, now, where, to, for,
"hardware" nearest neighbors: computer, program, system, software, designed, systems, technology, current,
"american" nearest neighbors: actor, d, writer, b, born, singer, author, robert,
"britain" nearest neighbors: great, established, government, england, country, in, from, state,
Step 2610000, Average Loss= 5.7291
Step 2620000, Average Loss= 5.6412
Step 2630000, Average Loss= 5.7485
Step 2640000, Average Loss= 5.5833
Step 2650000, Average Loss= 5.6548
Step 2660000, Average Loss= 5.7159
Step 2670000, Average Loss= 5.6569
Step 2680000, Average Loss= 5.6080
Step 2690000, Average Loss= 5.7037
Step 2700000, Average Loss= 5.6360
Step 2710000, Average Loss= 5.6707
Step 2720000, Average Loss= 5.6811
Step 2730000, Average Loss= 5.6237
Step 2740000, Average Loss= 5.7050
Step 2750000, Average Loss= 5.6991
Step 2760000, Average Loss= 5.6691
Step 2770000, Average Loss= 5.7057
Step 2780000, Average Loss= 5.6162
Step 2790000, Average Loss= 5.6484
Step 2800000, Average Loss= 5.6627
Evaluation...
"five" nearest neighbors: four, six, three, seven, eight, nine, two, one,
"of" nearest neighbors: the, in, following, including, part, and, from, under,
"going" nearest neighbors: again, before, little, away, once, when, eventually, then,
"hardware" nearest neighbors: computer, system, software, program, systems, designed, for, design,
"american" nearest neighbors: actor, writer, singer, author, born, robert, d, john,
"britain" nearest neighbors: established, england, great, government, france, army, the, throughout,
Step 2810000, Average Loss= 5.5900
Step 2820000, Average Loss= 5.7053
Step 2830000, Average Loss= 5.6064
Step 2840000, Average Loss= 5.6891
Step 2850000, Average Loss= 5.5571
Step 2860000, Average Loss= 5.4490
Step 2870000, Average Loss= 5.5428
Step 2880000, Average Loss= 5.6832
Step 2890000, Average Loss= 5.5973
Step 2900000, Average Loss= 5.5816
Step 2910000, Average Loss= 5.5647
Step 2920000, Average Loss= 5.6001
Step 2930000, Average Loss= 5.6459
Step 2940000, Average Loss= 5.5622
Step 2950000, Average Loss= 5.5707
Step 2960000, Average Loss= 5.6492
Step 2970000, Average Loss= 5.5633
Step 2980000, Average Loss= 5.6323
Step 2990000, Average Loss= 5.5440
Step 3000000, Average Loss= 5.6209
Evaluation...
"five" nearest neighbors: four, three, six, eight, seven, two, zero, one,
"of" nearest neighbors: the, in, and, including, group, includes, part, from,
"going" nearest neighbors: once, again, when, quickly, before, eventually, little, had,
"hardware" nearest neighbors: computer, system, software, designed, program, simple, systems, sound,
"american" nearest neighbors: canadian, english, author, german, french, british, irish, australian,
"britain" nearest neighbors: established, england, great, government, throughout, france, british, northern,
</pre>
</div>
</div>
</div>
</div>
</div>
</body>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: 'center'," +
        "    displayIndent: '0em'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['$','$'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        " linebreaks: { automatic: true, width: '95% container' }, " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
        "    } " +
        "}); ";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>

                </div>
                <footer class="text-right">
                    <p>- niult</p>
                </footer>
            </div>
        </div>
    </article>
</section>
<footer>
    <hr>
    <div class="row">
        <div class="col-lg-9 text-center">
            <p><small>
                Built by <a href="http://docs.getpelican.com/en/latest">Pelican</a> / <a href="https://github.com/ingwinlu/pelican-twitchy">pelican-twitchy</a>
                &middot;                    &copy; 2018 niult
            </small></p>
        </div>
    </div>
</footer>            </div>
        </div>
        <!-- /#page-content-wrapper -->
    </div>
    <!-- /#wrapper -->
    <!-- jQuery Version 1.11.2 -->
    <script src="../../../theme/js/jquery-1.11.2.min.js"></script>
    <!-- Bootstrap Core JavaScript -->
    <script src="../../../theme/js/bootstrap.min.js"></script>
    <!-- twitchy Script -->
    <script src="../../../theme/js/pelican_twitchy.min.js"></script>

</body>
</html>