<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!-->
<html class="no-js" lang="en"><!--<![endif]-->
    <head>
<meta charset="utf-8">
<title>魑魅魍魉</title>
<meta name="author" content="niult">




<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1">


    <link href="./favicon.png" rel="icon">

<link href="./theme/css/main.css" media="screen, projection"
      rel="stylesheet" type="text/css">

<link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic"
      rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic"
      rel="stylesheet" type="text/css">
</head>

<body>
<header role="banner"><hgroup>
  <h1><a href="./">魑魅魍魉</a></h1>
</hgroup></header>
<nav role="navigation">    <ul class="subscription" data-subscription="rss">
    </ul>


<ul class="main-navigation">
            <li >
                <a href="./category/01chang yong gong ju.html">01常用工具</a>
            </li>
            <li >
                <a href="./category/02.wo ai du shu.html">02.我爱读书</a>
            </li>
            <li >
                <a href="./category/algorithms.html">Algorithms</a>
            </li>
            <li >
                <a href="./category/book.html">Book</a>
            </li>
            <li >
                <a href="./category/book-pydata.html">Book-pydata</a>
            </li>
            <li >
                <a href="./category/deep-learning-with-python.html">Deep-learning-with-python</a>
            </li>
            <li >
                <a href="./category/ji qi xue xi shi zhan.html">机器学习实战</a>
            </li>
            <li >
                <a href="./category/ling ji chu ru men shen du xue xi.html">零基础入门深度学习</a>
            </li>
            <li >
                <a href="./category/shen du xue xi.html">深度学习</a>
            </li>
            <li >
                <a href="./category/shu xue ji chu.html">数学基础</a>
            </li>
            <li >
                <a href="./category/tf-example.html">Tf-example</a>
            </li>
            <li >
                <a href="./category/tool1.html">Tool1</a>
            </li>
            <li >
                <a href="./category/tool2.html">Tool2</a>
            </li>
            <li >
                <a href="./category/tools.html">Tools</a>
            </li>
            <li >
                <a href="./category/tui jian xi tong.html">推荐系统</a>
            </li>
            <li >
                <a href="./category/wen ben wa jue.html">文本挖掘</a>
            </li>
</ul>
</nav>
<div id="main">
    <div id="content">
    <div class="blog-index">
                <article>
<header>
        <h1 class="entry-title">
            <a href="./pages/2019/01/algorithms-logistic-code.html">algorithms-logistic-code</a>
        </h1>
    <p class="meta">
<time datetime="2019-01-16T00:00:00+08:00" pubdate>2019-01-16 00:00</time>    </p>
</header>

    <div class="entry-content">
<div class="cell border-box-sizing code_cell rendered">
<div class="input"></div>
        <footer>
            <a rel="full-article" href="./pages/2019/01/algorithms-logistic-code.html">Read On &crarr;</a>
        </footer>


                </article>
                <article>
<header>
        <h1 class="entry-title">
            <a href="./pages/2019/01/algorithms-logistic-li-lun.html">algorithms-logistic-理论</a>
        </h1>
    <p class="meta">
<time datetime="2019-01-16T00:00:00+08:00" pubdate>2019-01-16 00:00</time>    </p>
</header>

    <div class="entry-content">
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="逻辑回归">&#36923;&#36753;&#22238;&#24402;<a class="anchor-link" href="#逻辑回归">&#182;</a></h1><p>该算法尝试为<strong>给定一个输入特征的线性组合的一个二元变量的结果</strong>建模。</p>
<p>举个例子，可以根据候选人为竞选花费的金钱、时间等信息预测选举结果。</p>
<p>逻辑回归工作流程——</p>
<p>给定：</p>
<ul>
<li>数据集合 $\{(x^{(1)},y^{(1)},\ldots,(x^{(m)},y^{(m)})\}$</li>
<li>每个 $x^{(i)}$ 都是 $d$ 维向量 $x^{(i)}=(x_1^{(i)},\ldots,x_d^{(i)})$</li>
<li>每个 $y^{(i)}$ 都是一个二元目标变量 $y^{(i)} \in \{0,1\}$</li>
</ul>
<p>逻辑回归模型可以用非常简单的神经网络表达：</p>
<ul>
<li>拥有一个实值权重向量 $w=(w^{(1)},\ldots,w^{(d)})$</li>
<li>拥有一个实值偏置 $b$</li>
<li>采用sigmoid函数作为激活函数</li>
</ul>
<p>与<a href="./线性回归.ipynb">线性回归</a>不同，逻辑回归没有封闭解。</p>
<p>但是成本函数是凸的（<em>convex</em>），因此可以采用梯度下降方法来训练该模型。</p>
<p>事实上，梯度下降（或其余任何最优化算法）保证找到全局最小（如果学习率足够小且训练迭代次数足够多）。</p>
<p>训练步骤：</p>
<p>一开始模型参数是初始化了的，接下来重复指定的训练迭代次数或直到参数收敛。</p>
<ul>
<li><p>第一步</p>
<p>把权重向量和偏置初始化为零（或小的随机数）</p>
</li>
<li><p>第二步</p>
<p>计算一个输入特性和权重的线性组合。</p>
<p>采用向量化和广播所有训练样本可一次性搞定： $a=X \cdot w + b$</p>
<p>这里 $X$ 是矩阵 $(n_{samples},n_{features})$ 保佑全部训练样本，而 $\cdot$ 表示点积。</p>
</li>
<li><p>第三步</p>
<p>应用sigmoid激活函数，返回值在0和1之间： $$\hat y=\sigma (a)=\frac{1}{1 + \exp(-a)}$$</p></div>
        <footer>
            <a rel="full-article" href="./pages/2019/01/algorithms-logistic-li-lun.html">Read On &crarr;</a>
        </footer>


                </article>
                <article>
<header>
        <h1 class="entry-title">
            <a href="./pages/2019/01/algorithms-lr.html">algorithms-LR</a>
        </h1>
    <p class="meta">
<time datetime="2019-01-16T00:00:00+08:00" pubdate>2019-01-16 00:00</time>    </p>
</header>

    <div class="entry-content">
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="例1">&#20363;1<a class="anchor-link" href="#例1">&#182;</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input"></div>
        <footer>
            <a rel="full-article" href="./pages/2019/01/algorithms-lr.html">Read On &crarr;</a>
        </footer>


                </article>
                <article>
<header>
        <h1 class="entry-title">
            <a href="./pages/2019/01/algorithms-lr3.html">algorithms-LR3</a>
        </h1>
    <p class="meta">
<time datetime="2019-01-16T00:00:00+08:00" pubdate>2019-01-16 00:00</time>    </p>
</header>

    <div class="entry-content">
<div class="cell border-box-sizing code_cell rendered">
<div class="input"></div>
        <footer>
            <a rel="full-article" href="./pages/2019/01/algorithms-lr3.html">Read On &crarr;</a>
        </footer>


                </article>
                <article>
<header>
        <h1 class="entry-title">
            <a href="./pages/2019/01/algorithms-naivebayes.html">algorithms-naiveBayes</a>
        </h1>
    <p class="meta">
<time datetime="2019-01-16T00:00:00+08:00" pubdate>2019-01-16 00:00</time>    </p>
</header>

    <div class="entry-content">
<div class="cell border-box-sizing code_cell rendered">
<div class="input"></div>
        <footer>
            <a rel="full-article" href="./pages/2019/01/algorithms-naivebayes.html">Read On &crarr;</a>
        </footer>


                </article>
                <article>
<header>
        <h1 class="entry-title">
            <a href="./pages/2019/01/algorithms-pca.html">algorithms-PCA</a>
        </h1>
    <p class="meta">
<time datetime="2019-01-16T00:00:00+08:00" pubdate>2019-01-16 00:00</time>    </p>
</header>

    <div class="entry-content">
<div class="cell border-box-sizing code_cell rendered">
<div class="input"></div>
        <footer>
            <a rel="full-article" href="./pages/2019/01/algorithms-pca.html">Read On &crarr;</a>
        </footer>


                </article>
                <article>
<header>
        <h1 class="entry-title">
            <a href="./pages/2019/01/algorithms-pca2.html">algorithms-PCA2</a>
        </h1>
    <p class="meta">
<time datetime="2019-01-16T00:00:00+08:00" pubdate>2019-01-16 00:00</time>    </p>
</header>

    <div class="entry-content">
<div class="cell border-box-sizing code_cell rendered">
<div class="input"></div>
        <footer>
            <a rel="full-article" href="./pages/2019/01/algorithms-pca2.html">Read On &crarr;</a>
        </footer>


                </article>
                <article>
<header>
        <h1 class="entry-title">
            <a href="./pages/2019/01/algorithms-rnnwang-luo.html">algorithms-RNN网络</a>
        </h1>
    <p class="meta">
<time datetime="2019-01-16T00:00:00+08:00" pubdate>2019-01-16 00:00</time>    </p>
</header>

    <div class="entry-content">
<div class="cell border-box-sizing code_cell rendered">
<div class="input"></div>
        <footer>
            <a rel="full-article" href="./pages/2019/01/algorithms-rnnwang-luo.html">Read On &crarr;</a>
        </footer>


                </article>
                <article>
<header>
        <h1 class="entry-title">
            <a href="./pages/2019/01/algorithms-rnnwang-luo-2.html">algorithms-RNN网络2</a>
        </h1>
    <p class="meta">
<time datetime="2019-01-16T00:00:00+08:00" pubdate>2019-01-16 00:00</time>    </p>
</header>

    <div class="entry-content">
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>零基础入门深度学习(5) - 循环神经网络
机器学习 深度学习入门</p>
<p><img src="data/image/20180926/2256672-06627c71f0d8c0dc.jpg" alt="image"></p>
<blockquote><p>无论即将到来的是大数据时代还是人工智能时代，亦或是传统行业使用人工智能在云上处理大数据的时代，作为一个有理想有追求的程序员，不懂深度学习（Deep Learning）这个超热的技术，会不会感觉马上就out了？现在救命稻草来了，《零基础入门深度学习》系列文章旨在讲帮助爱编程的你从零基础达到入门级水平。零基础意味着你不需要太多的数学知识，只要会写程序就行了，没错，这是专门为程序员写的文章。虽然文中会有很多公式你也许看不懂，但同时也会有更多的代码，程序员的你一定能看懂的（我周围是一群狂热的Clean Code程序员，所以我写的代码也不会很差）。</p>
</blockquote>
<p>往期回顾
全连接神经网络和卷积神经网络，他们都只能单独的取处理一个个的输入，前一个输入和后一个输入是完全没有关系的。但是，某些任务需要能够更好的处理序列的信息，即前面的输入和后面的输入是有关系的。比如，当我们在理解一句话意思时，孤立的理解这句话的每个词是不够的，我们需要处理这些词连接起来的整个序列；当我们处理视频的时候，我们也不能只单独的去分析每一帧，而要分析这些帧连接起来的整个序列。这时，就需要用到深度学习领域中另一类非常重要神经网络：循环神经网络(Recurrent Neural Network)。RNN种类很多，也比较绕脑子。不过读者不用担心，本文将一如既往的对复杂的东西剥茧抽丝，帮助您理解RNNs以及它的训练算法，并动手实现一个循环神经网络。</p>
<p>语言模型
RNN是在自然语言处理领域中最先被用起来的，比如，RNN可以为语言模型来建模。那么，什么是语言模型呢？</p>
<p>我们可以和电脑玩一个游戏，我们写出一个句子前面的一些词，然后，让电脑帮我们写下接下来的一个词。比如下面这句：</p>
<blockquote><p>我昨天上学迟到了，老师批评了<em>__</em>。</p>
</blockquote>
<p>我们给电脑展示了这句话前面这些词，然后，让电脑写下接下来的一个词。在这个例子中，接下来的这个词最有可能是『我』，而不太可能是『小明』，甚至是『吃饭』。</p>
<p>语言模型就是这样的东西：给定一个一句话前面的部分，预测接下来最有可能的一个词是什么。</p>
<p>语言模型是对一种语言的特征进行建模，它有很多很多用处。比如在语音转文本(STT)的应用中，声学模型输出的结果，往往是若干个可能的候选词，这时候就需要语言模型来从这些候选词中选择一个最可能的。当然，它同样也可以用在图像到文本的识别中(OCR)。</p>
<p>使用RNN之前，语言模型主要是采用N-Gram。N可以是一个自然数，比如2或者3。它的含义是，假设一个词出现的概率只与前面N个词相关。我们以2-Gram为例。首先，对前面的一句话进行切词：</p>
<blockquote><p>我 昨天 上学 迟到 了 ，老师 批评 了 <em>__</em>。</p>
</blockquote>
<p>如果用2-Gram进行建模，那么电脑在预测的时候，只会看到前面的『了』，然后，电脑会在语料库中，搜索『了』后面最可能的一个词。不管最后电脑选的是不是『我』，我们都知道这个模型是不靠谱的，因为『了』前面说了那么一大堆实际上是没有用到的。如果是3-Gram模型呢，会搜索『批评了』后面最可能的词，感觉上比2-Gram靠谱了不少，但还是远远不够的。因为这句话最关键的信息『我』，远在9个词之前！</p>
<p>现在读者可能会想，可以提升继续提升N的值呀，比如4-Gram、5-Gram.......。实际上，这个想法是没有实用性的。因为我们想处理任意长度的句子，N设为多少都不合适；另外，模型的大小和N的关系是指数级的，4-Gram模型就会占用海量的存储空间。</p>
<p>所以，该轮到RNN出场了，RNN理论上可以往前看(往后看)任意多个词。</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="循环神经网络是啥">&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#26159;&#21861;<a class="anchor-link" href="#循环神经网络是啥">&#182;</a></h1><p>循环神经网络种类繁多，我们先从最简单的基本循环神经网络开始吧。</p>
<h2 id="基本循环神经网络">&#22522;&#26412;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;<a class="anchor-link" href="#基本循环神经网络">&#182;</a></h2><p>下图是一个简单的循环神经网络如，它由输入层、一个隐藏层和一个输出层组成：</p>
<p>纳尼？！相信第一次看到这个玩意的读者内心和我一样是崩溃的。因为循环神经网络实在是太难画出来了，网上所有大神们都不得不用了这种抽象艺术手法。不过，静下心来仔细看看的话，其实也是很好理解的。如果把上面有W的那个带箭头的圈去掉，它就变成了最普通的全连接神经网络。x是一个向量，它表示输入层的值（这里面没有画出来表示神经元节点的圆圈）；s是一个向量，它表示隐藏层的值（这里隐藏层面画了一个节点，你也可以想象这一层其实是多个节点，节点数与向量s的维度相同）；U是输入层到隐藏层的权重矩阵；o也是一个向量，它表示输出层的值；V是隐藏层到输出层的权重矩阵。那么，现在我们来看看W是什么。循环神经网络的隐藏层的值s不仅仅取决于当前这次的输入x，还取决于上一次隐藏层的值s。权重矩阵 W就是隐藏层上一次的值作为这一次的输入的权重。</p>
<p>如果我们把上面的图展开，循环神经网络也可以画成下面这个样子：</p>
<p>现在看上去就比较清楚了，这个网络在t时刻接收到输入之后，隐藏层的值是，输出值是。关键一点是，的值不仅仅取决于，还取决于。我们可以用下面的公式来表示循环神经网络的计算方法：</p>
<p>式
式
式1是输出层的计算公式，输出层是一个全连接层，也就是它的每个节点都和隐藏层的每个节点相连。V是输出层的权重矩阵，g是激活函数。式2是隐藏层的计算公式，它是循环层。U是输入x的权重矩阵，W是上一次的值作为这一次的输入的权重矩阵，f是激活函数。</p>
<p>从上面的公式我们可以看出，循环层和全连接层的区别就是循环层多了一个权重矩阵 W。</p>
<p>如果反复把式2带入到式1，我们将得到：</p>
<p>从上面可以看出，循环神经网络的输出值，是受前面历次输入值、、、、...影响的，这就是为什么循环神经网络可以往前看任意多个输入值的原因。</p>
<p>双向循环神经网络
对于语言模型来说，很多时候光看前面的词是不够的，比如下面这句话：</p>
<p>我的手机坏了，我打算____一部新手机。</p>
<p>可以想象，如果我们只看横线前面的词，手机坏了，那么我是打算修一修？换一部新的？还是大哭一场？这些都是无法确定的。但如果我们也看到了横线后面的词是『一部新手机』，那么，横线上的词填『买』的概率就大得多了。</p>
<p>在上一小节中的基本循环神经网络是无法对此进行建模的，因此，我们需要双向循环神经网络，如下图所示：</p>
<p>当遇到这种从未来穿越回来的场景时，难免处于懵逼的状态。不过我们还是可以用屡试不爽的老办法：先分析一个特殊场景，然后再总结一般规律。我们先考虑上图中，的计算。</p>
<p>从上图可以看出，双向卷积神经网络的隐藏层要保存两个值，一个A参与正向计算，另一个值A'参与反向计算。最终的输出值取决于和。其计算方法为：</p>
<p>和则分别计算：</p>
<p>现在，我们已经可以看出一般的规律：正向计算时，隐藏层的值与有关；反向计算时，隐藏层的值与有关；最终的输出取决于正向和反向计算的加和。现在，我们仿照式1和式2，写出双向循环神经网络的计算方法：</p>
<p>从上面三个公式我们可以看到，正向计算和反向计算不共享权重，也就是说U和U'、W和W'、V和V'都是不同的权重矩阵。</p>
<p>深度循环神经网络
前面我们介绍的循环神经网络只有一个隐藏层，我们当然也可以堆叠两个以上的隐藏层，这样就得到了深度循环神经网络。如下图所示：</p>
<p>我们把第i个隐藏层的值表示为、，则深度循环神经网络的计算方式可以表示为：</p>
<p>循环神经网络的训练
循环神经网络的训练算法：BPTT
BPTT算法是针对循环层的训练算法，它的基本原理和BP算法是一样的，也包含同样的三个步骤：</p>
<p>前向计算每个神经元的输出值；
反向计算每个神经元的误差项值，它是误差函数E对神经元j的加权输入的偏导数；
计算每个权重的梯度。
最后再用随机梯度下降算法更新权重。</p>
<p>循环层如下图所示：</p>
<p>前向计算
使用前面的式2对循环层进行前向计算：</p>
<p>注意，上面的、、都是向量，用黑体字母表示；而U、V是矩阵，用大写字母表示。向量的下标表示时刻，例如，表示在t时刻向量s的值。</p>
<p>我们假设输入向量x的维度是m，输出向量s的维度是n，则矩阵U的维度是，矩阵W的维度是。下面是上式展开成矩阵的样子，看起来更直观一些：</p>
<p>在这里我们用手写体字母表示向量的一个元素，它的下标表示它是这个向量的第几个元素，它的上标表示第几个时刻。例如，表示向量s的第j个元素在t时刻的值。表示输入层第i个神经元到循环层第j个神经元的权重。表示循环层第t-1时刻的第i个神经元到循环层第t个时刻的第j个神经元的权重。</p>
<p>误差项的计算
BTPP算法将第l层t时刻的误差项值沿两个方向传播，一个方向是其传递到上一层网络，得到，这部分只和权重矩阵U有关；另一个是方向是将其沿时间线传递到初始时刻，得到，这部分只和权重矩阵W有关。</p>
<p>我们用向量表示神经元在t时刻的加权输入，因为：</p>
<p>因此：</p>
<p>我们用a表示列向量，用表示行向量。上式的第一项是向量函数对向量求导，其结果为Jacobian矩阵：</p>
<p>同理，上式第二项也是一个Jacobian矩阵：</p>
<p>其中，diag[a]表示根据向量a创建一个对角矩阵，即</p>
<p>最后，将两项合在一起，可得：</p>
<p>上式描述了将沿时间往前传递一个时刻的规律，有了这个规律，我们就可以求得任意时刻k的误差项：</p>
<p>式
式3就是将误差项沿时间反向传播的算法。</p>
<p>循环层将误差项反向传递到上一层网络，与普通的全连接层是完全一样的，这在前面的文章零基础入门深度学习(3) - 神经网络和反向传播算法中已经详细讲过了，在此仅简要描述一下。</p>
<p>循环层的加权输入与上一层的加权输入关系如下：</p>
<p>上式中是第l层神经元的加权输入(假设第l层是循环层)；是第l-1层神经元的加权输入；是第l-1层神经元的输出；是第l-1层的激活函数。</p>
<p>所以，</p>
<p>式
式4就是将误差项传递到上一层算法。</p>
<p>权重梯度的计算
现在，我们终于来到了BPTT算法的最后一步：计算每个权重的梯度。</p>
<p>首先，我们计算误差函数E对权重矩阵W的梯度。</p>
<p>上图展示了我们到目前为止，在前两步中已经计算得到的量，包括每个时刻t 循环层的输出值，以及误差项。</p>
<p>回忆一下我们在文章零基础入门深度学习(3) - 神经网络和反向传播算法介绍的全连接网络的权重梯度计算算法：只要知道了任意一个时刻的误差项，以及上一个时刻循环层的输出值，就可以按照下面的公式求出权重矩阵在t时刻的梯度：</p>
<p>式
在式5中，表示t时刻误差项向量的第i个分量；表示t-1时刻循环层第i个神经元的输出值。</p>
<p>我们下面可以简单推导一下式5。</p>
<p>我们知道：</p>
<p>因为对W求导与无关，我们不再考虑。现在，我们考虑对权重项求导。通过观察上式我们可以看到只与有关，所以：</p>
<p>按照上面的规律就可以生成式5里面的矩阵。</p>
<p>我们已经求得了权重矩阵W在t时刻的梯度，最终的梯度是各个时刻的梯度之和：</p>
<p>式
式6就是计算循环层权重矩阵W的梯度的公式。</p>
<p>----------数学公式超高能预警----------</p>
<p>前面已经介绍了的计算方法，看上去还是比较直观的。然而，读者也许会困惑，为什么最终的梯度是各个时刻的梯度之和呢？我们前面只是直接用了这个结论，实际上这里面是有道理的，只是这个数学推导比较绕脑子。感兴趣的同学可以仔细阅读接下来这一段，它用到了矩阵对矩阵求导、张量与向量相乘运算的一些法则。</p>
<p>我们还是从这个式子开始：</p>
<p>因为与W完全无关，我们把它看做常量。现在，考虑第一个式子加号右边的部分，因为W和都是W的函数，因此我们要用到大学里面都学过的导数乘法运算：</p>
<p>因此，上面第一个式子写成：</p>
<p>我们最终需要计算的是：</p>
<p>式
我们先计算式7加号左边的部分。是矩阵对矩阵求导，其结果是一个四维张量(tensor)，如下所示：</p>
<p>接下来，我们知道，它是一个列向量。我们让上面的四维张量与这个向量相乘，得到了一个三维张量，再左乘行向量，最终得到一个矩阵：</p>
<p>接下来，我们计算式7加号右边的部分：</p>
<p>于是，我们得到了如下递推公式：</p>
<p>这样，我们就证明了：最终的梯度是各个时刻的梯度之和。</p>
<p>----------数学公式超高能预警解除----------</p>
<p>同权重矩阵W类似，我们可以得到权重矩阵U的计算方法。</p>
<p>式
式8是误差函数在t时刻对权重矩阵U的梯度。和权重矩阵W一样，最终的梯度也是各个时刻的梯度之和：</p>
<p>具体的证明这里就不再赘述了，感兴趣的读者可以练习推导一下。</p>
<p>RNN的梯度爆炸和消失问题
不幸的是，实践中前面介绍的几种RNNs并不能很好的处理较长的序列。一个主要的原因是，RNN在训练中很容易发生梯度爆炸和梯度消失，这导致训练时梯度不能在较长序列中一直传递下去，从而使RNN无法捕捉到长距离的影响。</p>
<p>为什么RNN会产生梯度爆炸和消失问题呢？我们接下来将详细分析一下原因。我们根据式3可得：</p>
<p>上式的定义为矩阵的模的上界。因为上式是一个指数函数，如果t-k很大的话（也就是向前看很远的时候），会导致对应的误差项的值增长或缩小的非常快，这样就会导致相应的梯度爆炸和梯度消失问题（取决于大于1还是小于1）。</p>
<p>通常来说，梯度爆炸更容易处理一些。因为梯度爆炸的时候，我们的程序会收到NaN错误。我们也可以设置一个梯度阈值，当梯度超过这个阈值的时候可以直接截取。</p>
<p>梯度消失更难检测，而且也更难处理一些。总的来说，我们有三种方法应对梯度消失问题：</p>
<p>合理的初始化权重值。初始化权重，使每个神经元尽可能不要取极大或极小值，以躲开梯度消失的区域。
使用relu代替sigmoid和tanh作为激活函数。原理请参考上一篇文章零基础入门深度学习(4) - 卷积神经网络的激活函数一节。
使用其他结构的RNNs，比如长短时记忆网络（LTSM）和Gated Recurrent Unit（GRU），这是最流行的做法。我们将在以后的文章中介绍这两种网络。
RNN的应用举例——基于RNN的语言模型
现在，我们介绍一下基于RNN语言模型。我们首先把词依次输入到循环神经网络中，每输入一个词，循环神经网络就输出截止到目前为止，下一个最可能的词。例如，当我们依次输入：</p>
<p>我 昨天 上学 迟到 了</p></div>
        <footer>
            <a rel="full-article" href="./pages/2019/01/algorithms-rnnwang-luo-2.html">Read On &crarr;</a>
        </footer>


                </article>
                <article>
<header>
        <h1 class="entry-title">
            <a href="./pages/2019/01/algorithms-shi-fen-zhong-shang-shou-xgboost.html">algorithms-十分钟上手xgboost</a>
        </h1>
    <p class="meta">
<time datetime="2019-01-16T00:00:00+08:00" pubdate>2019-01-16 00:00</time>    </p>
</header>

    <div class="entry-content">
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="xgboost">xgboost<a class="anchor-link" href="#xgboost">&#182;</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="参考网址">&#21442;&#32771;&#32593;&#22336;<a class="anchor-link" href="#参考网址">&#182;</a></h2><p>1 XGBoost参数调优完全指南（附Python代码） <a href="https://www.cnblogs.com/mfryf/p/6293814.html">https://www.cnblogs.com/mfryf/p/6293814.html</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="XGBoost的参数">XGBoost&#30340;&#21442;&#25968;<a class="anchor-link" href="#XGBoost的参数">&#182;</a></h1><p>XGBoost的作者把所有的参数分成了三类：<br>
1、通用参数：宏观函数控制。<br>
2、Booster参数：控制每一步的booster(tree/regression)。<br>
3、学习目标参数：控制训练目标的表现。<br>
在这里我会类比GBM来讲解，所以作为一种基础知识。</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="通用参数">&#36890;&#29992;&#21442;&#25968;<a class="anchor-link" href="#通用参数">&#182;</a></h2><p>这些参数用来控制XGBoost的宏观功能。</p>
<h3 id="booster[默认gbtree]">booster[&#40664;&#35748;gbtree]<a class="anchor-link" href="#booster[默认gbtree]">&#182;</a></h3><p>选择每次迭代的模型，有两种选择：
gbtree：基于树的模型
gbliner：线性模型</p>
<h3 id="silent[默认0]">silent[&#40664;&#35748;0]<a class="anchor-link" href="#silent[默认0]">&#182;</a></h3><p>当这个参数值为1时，静默模式开启，不会输出任何信息。 一般这个参数就保持默认的0，因为这样能帮我们更好地理解模型。</p>
<h3 id="nthread[默认值为最大可能的线程数]">nthread[&#40664;&#35748;&#20540;&#20026;&#26368;&#22823;&#21487;&#33021;&#30340;&#32447;&#31243;&#25968;]<a class="anchor-link" href="#nthread[默认值为最大可能的线程数]">&#182;</a></h3><p>这个参数用来进行多线程控制，应当输入系统的核数。 如果你希望使用CPU全部的核，那就不要输入这个参数，算法会自动检测它。
还有两个参数，XGBoost会自动设置，目前你不用管它。接下来咱们一起看booster参数。</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="booster参数">booster&#21442;&#25968;<a class="anchor-link" href="#booster参数">&#182;</a></h2><p>尽管有两种booster可供选择，我这里只介绍tree booster，因为它的表现远远胜过linear booster，所以linear booster很少用到。</p>
<h3 id="eta[默认0.3]">eta[&#40664;&#35748;0.3]<a class="anchor-link" href="#eta[默认0.3]">&#182;</a></h3><p>和GBM中的 learning rate 参数类似。 通过减少每一步的权重，可以提高模型的鲁棒性。 典型值为0.01-0.2。</p>
<h3 id="min_child_weight[默认1]">min_child_weight[&#40664;&#35748;1]<a class="anchor-link" href="#min_child_weight[默认1]">&#182;</a></h3><p>决定最小叶子节点样本权重和。 和GBM的 min_child_leaf 参数类似，但不完全一样。XGBoost的这个参数是最小样本权重的和，而GBM参数是最小样本总数。 这个参数用于避免过拟合。当它的值较大时，可以避免模型学习到局部的特殊样本。 但是如果这个值过高，会导致欠拟合。这个参数需要使用CV来调整。</p>
<h2 id="max_depth[默认6]">max_depth[&#40664;&#35748;6]<a class="anchor-link" href="#max_depth[默认6]">&#182;</a></h2><p>和GBM中的参数相同，这个值为树的最大深度。 这个值也是用来避免过拟合的。max_depth越大，模型会学到更具体更局部的样本。 需要使用CV函数来进行调优。 典型值：3-10</p>
<h3 id="max_leaf_nodes">max_leaf_nodes<a class="anchor-link" href="#max_leaf_nodes">&#182;</a></h3><p>树上最大的节点或叶子的数量。 可以替代max_depth的作用。因为如果生成的是二叉树，一个深度为n的树最多生成n2个叶子。 如果定义了这个参数，GBM会忽略max_depth参数。</p>
<h3 id="gamma[默认0]">gamma[&#40664;&#35748;0]<a class="anchor-link" href="#gamma[默认0]">&#182;</a></h3><p>在节点分裂时，只有分裂后损失函数的值下降了，才会分裂这个节点。Gamma指定了节点分裂所需的最小损失函数下降值。 这个参数的值越大，算法越保守。这个参数的值和损失函数息息相关，所以是需要调整的。</p>
<h3 id="max_delta_step[默认0]">max_delta_step[&#40664;&#35748;0]<a class="anchor-link" href="#max_delta_step[默认0]">&#182;</a></h3><p>这参数限制每棵树权重改变的最大步长。如果这个参数的值为0，那就意味着没有约束。如果它被赋予了某个正值，那么它会让这个算法更加保守。 通常，这个参数不需要设置。但是当各类别的样本十分不平衡时，它对逻辑回归是很有帮助的。 这个参数一般用不到，但是你可以挖掘出来它更多的用处。</p>
<h3 id="subsample[默认1]">subsample[&#40664;&#35748;1]<a class="anchor-link" href="#subsample[默认1]">&#182;</a></h3><p>和GBM中的subsample参数一模一样。这个参数控制对于每棵树，随机采样的比例。 减小这个参数的值，算法会更加保守，避免过拟合。但是，如果这个值设置得过小，它可能会导致欠拟合。 典型值：0.5-1</p>
<h3 id="colsample_bytree[默认1]">colsample_bytree[&#40664;&#35748;1]<a class="anchor-link" href="#colsample_bytree[默认1]">&#182;</a></h3><p>和GBM里面的max_features参数类似。用来控制每棵随机采样的列数的占比(每一列是一个特征)。 典型值：0.5-1</p>
<h3 id="colsample_bylevel[默认1]">colsample_bylevel[&#40664;&#35748;1]<a class="anchor-link" href="#colsample_bylevel[默认1]">&#182;</a></h3><p>用来控制树的每一级的每一次分裂，对列数的采样的占比。 我个人一般不太用这个参数，因为subsample参数和colsample_bytree参数可以起到相同的作用。但是如果感兴趣，可以挖掘这个参数更多的用处。</p></div>
        <footer>
            <a rel="full-article" href="./pages/2019/01/algorithms-shi-fen-zhong-shang-shou-xgboost.html">Read On &crarr;</a>
        </footer>


                </article>
                <article>
<header>
        <h1 class="entry-title">
            <a href="./pages/2019/01/algorithms-svd.html">algorithms-SVD</a>
        </h1>
    <p class="meta">
<time datetime="2019-01-16T00:00:00+08:00" pubdate>2019-01-16 00:00</time>    </p>
</header>

    <div class="entry-content">
<div class="cell border-box-sizing code_cell rendered">
<div class="input"></div>
        <footer>
            <a rel="full-article" href="./pages/2019/01/algorithms-svd.html">Read On &crarr;</a>
        </footer>


                </article>
                <article>
<header>
        <h1 class="entry-title">
            <a href="./pages/2019/01/algorithms-wen-ben-wa-jue-yu-chu-li-zhi-tf-idf.html">algorithms-文本挖掘预处理之TF-IDF</a>
        </h1>
    <p class="meta">
<time datetime="2019-01-16T00:00:00+08:00" pubdate>2019-01-16 00:00</time>    </p>
</header>

    <div class="entry-content">
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>在<a href="http://www.cnblogs.com/pinard/p/6688348.html">文本挖掘预处理之向量化与Hash Trick</a>中我们讲到在文本挖掘的预处理中，向量化之后一般都伴随着TF-IDF的处理，那么什么是TF-IDF，为什么一般我们要加这一步预处理呢？这里就对TF-IDF的原理做一个总结。</p>
<h1 id="文本向量化特征的不足">&#25991;&#26412;&#21521;&#37327;&#21270;&#29305;&#24449;&#30340;&#19981;&#36275;<a class="anchor-link" href="#文本向量化特征的不足">&#182;</a></h1><p>在将文本分词并向量化后，我们可以得到词汇表中每个词在各个文本中形成的词向量，比如在<a href="http://www.cnblogs.com/pinard/p/6688348.html">文本挖掘预处理之向量化与Hash Trick</a>这篇文章中，我们将下面4个短文本做了词频统计：</p>

<pre><code>corpus=["I come to China to travel", "This is a car polupar in China", "I love tea and Apple ", "The work is to write some papers in science"] 

</code></div>
        <footer>
            <a rel="full-article" href="./pages/2019/01/algorithms-wen-ben-wa-jue-yu-chu-li-zhi-tf-idf.html">Read On &crarr;</a>
        </footer>


                </article>
                <article>
<header>
        <h1 class="entry-title">
            <a href="./pages/2019/01/algorithms-wen-ben-zhu-ti-mo-xing-zhi-qian-zai-yu-yi-suo-yin-lsi.html">algorithms-文本主题模型之潜在语义索引(LSI)</a>
        </h1>
    <p class="meta">
<time datetime="2019-01-16T00:00:00+08:00" pubdate>2019-01-16 00:00</time>    </p>
</header>

    <div class="entry-content">
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="简介">&#31616;&#20171;<a class="anchor-link" href="#简介">&#182;</a></h1><p>在文本挖掘中，主题模型是比较特殊的一块，它的思想不同于我们常用的机器学习算法，因此这里我们需要专门来总结文本主题模型的算法。本文关注于潜在语义索引算法(LSI)的原理。</p>
<h1 id="文本主题模型的问题特点">&#25991;&#26412;&#20027;&#39064;&#27169;&#22411;&#30340;&#38382;&#39064;&#29305;&#28857;<a class="anchor-link" href="#文本主题模型的问题特点">&#182;</a></h1><p>在数据分析中，我们经常会进行非监督学习的聚类算法，它可以对我们的特征数据进行非监督的聚类。而主题模型也是非监督的算法，目的是得到文本按照主题的概率分布。从这个方面来说，主题模型和普通的聚类算法非常的类似。但是两者其实还是有区别的。</p>
<p>聚类算法关注于从样本特征的相似度方面将数据聚类。比如通过数据样本之间的欧式距离，曼哈顿距离的大小聚类等。而主题模型，顾名思义，就是对文字中隐含主题的一种建模方法。比如从“人民的名义”和“达康书记”这两个词我们很容易发现对应的文本有很大的主题相关度，但是如果通过词特征来聚类的话则很难找出，因为聚类方法不能考虑到到隐含的主题这一块。</p>
<p>那么如何找到隐含的主题呢？这个一个大问题。常用的方法一般都是基于统计学的生成方法。即假设以一定的概率选择了一个主题，然后以一定的概率选择当前主题的词。最后这些词组成了我们当前的文本。所有词的统计概率分布可以从语料库获得，具体如何以“一定的概率选择”，这就是各种具体的主题模型算法的任务了。</p>
<p>当然还有一些不是基于统计的方法，比如我们下面讲到的LSI。</p>
<h1 id="潜在语义索引(LSI)概述">&#28508;&#22312;&#35821;&#20041;&#32034;&#24341;(LSI)&#27010;&#36848;<a class="anchor-link" href="#潜在语义索引(LSI)概述">&#182;</a></h1><p>潜在语义索引(Latent Semantic Indexing,以下简称LSI)，有的文章也叫Latent Semantic Analysis（LSA）。其实是一个东西，后面我们统称LSI，它是一种简单实用的主题模型。LSI是基于奇异值分解（SVD）的方法来得到文本的主题的。而SVD及其应用我们在前面的文章也多次讲到，比如：<a href="http://www.cnblogs.com/pinard/p/6251584.html">奇异值分解(SVD)原理与在降维中的应用</a>和<a href="http://www.cnblogs.com/pinard/p/6351319.html">矩阵分解在协同过滤推荐算法中的应用</a>。如果大家对SVD还不熟悉，建议复习<a href="http://www.cnblogs.com/pinard/p/6251584.html">奇异值分解(SVD)原理与在降维中的应用</a>后再读下面的内容。</p>
<p>这里我们简要回顾下SVD：对于一个$m \times n$的矩阵$A$，可以分解为下面三个矩阵：</p>
<p>$$A_{m \times n} = U_{m \times m}\Sigma_{m \times n} V^T_{n \times n}$$</p>
<p>有时为了降低矩阵的维度到k，SVD的分解可以近似的写为：</p>
<p>$$A_{m \times n} \approx U_{m \times k}\Sigma_{k \times k} V^T_{k \times n}$$</p>
<p>如果把上式用到我们的主题模型，则SVD可以这样解释：我们输入的有m个文本，每个文本有n个词。而$A_{ij}$则对应第i个文本的第j个词的特征值，这里最常用的是基于预处理后的标准化TF-IDF值。k是我们假设的主题数，一般要比文本数少。SVD分解后，$U_{il}$对应第i个文本和第l个主题的相关度。$V_{jm}$对应第j个词和第m个词义的相关度。$\Sigma_{lm}$对应第l个主题和第m个词义的相关度。</p>
<p>也可以反过来解释：我们输入的有m个词，对应n个文本。而$A_{ij}$则对应第i个词档的第j个文本的特征值，这里最常用的是基于预处理后的标准化TF-IDF值。k是我们假设的主题数，一般要比文本数少。SVD分解后，$U_{il}$对应第i个词和第l个词义的相关度。$V_{jm}$对应第j个文本和第m个主题的相关度。$\Sigma_{lm}$对应第l个词义和第m个主题的相关度。</p>
<p>这样我们通过一次SVD，就可以得到文档和主题的相关度，词和词义的相关度以及词义和主题的相关度。</p>
<h1 id="LSI简单实例">LSI&#31616;&#21333;&#23454;&#20363;<a class="anchor-link" href="#LSI简单实例">&#182;</a></h1><p>这里举一个简单的LSI实例，假设我们有下面这个有11个词三个文本的词频TF对应矩阵如下：</p>
<p><img src="https://images2015.cnblogs.com/blog/1042406/201705/1042406-20170504134451664-1723370358.png" alt=""></p>
<p>这里我们没有使用预处理，也没有使用TF-IDF，在实际应用中最好使用预处理后的TF-IDF值矩阵作为输入。</p>
<p>我们假定对应的主题数为2，则通过SVD降维后得到的三矩阵为：</p>
<p><img src="https://images2015.cnblogs.com/blog/1042406/201705/1042406-20170504135321726-2116029824.png" alt=""></p>
<p>从矩阵$U_k$我们可以看到词和词义之间的相关性。而从$V_k$可以看到3个文本和两个主题的相关性。大家可以看到里面有负数，所以这样得到的相关度比较难解释。</p>
<h1 id="LSI用于文本相似度计算">LSI&#29992;&#20110;&#25991;&#26412;&#30456;&#20284;&#24230;&#35745;&#31639;<a class="anchor-link" href="#LSI用于文本相似度计算">&#182;</a></h1><p>在上面我们通过LSI得到的文本主题矩阵可以用于文本相似度计算。而计算方法一般是通过余弦相似度。比如对于上面的三文档两主题的例子。我们可以计算第一个文本和第二个文本的余弦相似度如下 ：$$sim(d1,d2) = \frac{(-0.4945)*(-0.6458) + (0.6492)*(-0.7194)}{\sqrt{(-0.4945)^2+0.6492^2}\sqrt{(-0.6458)^2+(-0.7194)^2}}$$</p></div>
        <footer>
            <a rel="full-article" href="./pages/2019/01/algorithms-wen-ben-zhu-ti-mo-xing-zhi-qian-zai-yu-yi-suo-yin-lsi.html">Read On &crarr;</a>
        </footer>


                </article>
                <article>
<header>
        <h1 class="entry-title">
            <a href="./pages/2019/01/algorithms-xgboost1.html">algorithms-xgboost1</a>
        </h1>
    <p class="meta">
<time datetime="2019-01-16T00:00:00+08:00" pubdate>2019-01-16 00:00</time>    </p>
</header>

    <div class="entry-content">
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="xgBoost">xgBoost<a class="anchor-link" href="#xgBoost">&#182;</a></h1><p><a href="https://www.cnblogs.com/mfryf/p/6238185.html">https://www.cnblogs.com/mfryf/p/6238185.html</a></p>
<p><a href="https://www.zhihu.com/question/41354392">https://www.zhihu.com/question/41354392</a></p>
<p><a href="https://blog.csdn.net/github_38414650/article/details/76061893">https://blog.csdn.net/github_38414650/article/details/76061893</a></p>
<p><a href="https://www.cnblogs.com/csyuan/p/6537255.html">https://www.cnblogs.com/csyuan/p/6537255.html</a></p>
<p><a href="https://www.cnblogs.com/infaraway/p/7890558.html">https://www.cnblogs.com/infaraway/p/7890558.html</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Gradient-Boosting-Tree-算法原理">Gradient Boosting Tree &#31639;&#27861;&#21407;&#29702;<a class="anchor-link" href="#Gradient-Boosting-Tree-算法原理">&#182;</a></h2><p>Friedman于论文”GreedyFunctionApproximation...”中最早提出GBDT</p>
<ul>
<li>其模型F定义为加法模型:</li>
</ul>
<p>$$F(x;w)=\sum^T_{t=0}{\alpha_ih_i}\big(x;w_i\big)=\sum^T_{t=0}f_i\big(x;w_i\big) $$</p>
<p>其中，$x$为输入样本，$h$为分类回归树，$w$是分类回归树的参数， $\alpha$是每棵树的权重。</p>
<ul>
<li>通过最小化损失函数求解最优模型:</li>
</ul>
<p>$$F^*=\arg_F{\min\sum^N_{i=0}L\Big(y_i,F\big(x_i;w\big)\Big)}$$</p>
<p>NP难问题 -&gt; 通过贪心法，迭代求局部最优解</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>输入：$\big(x_i,y_i\big),T,L$</p>
<p>1 初始化$f_0$</p>
<p>2 for t = 1 to T do
  2.1 计算响应：$\tilde {y_i}= -{\Bigg[\frac{\partial L\big(y_i,F(x_i) \big)}{\partial F(x_i)} \Bigg]}_{F(x)=F_{t-1}(x)},i=1,2,...,N$<br>
  2.2 学习第$t$棵树：$w^*=\arg_w{\min\sum\limits^N_{i=1}\Big(\tilde{y_i}-h_i\big(x_i;w \big) \Big)}$<br>
  2.3 line search找步长：
  $\rho^*=\arg_{\rho}\min\sum\limits^N_{i=1}L\Big(y_i,F_{t-1}(x_i)+\rho h_i\big(x_i;w^* \big) \Big)$<br>
  2.4 令$f_t=\rho^*h_i(x;w^*)$，更新模型  $F_t=F_{t-1}+f_t$</p></div>
        <footer>
            <a rel="full-article" href="./pages/2019/01/algorithms-xgboost1.html">Read On &crarr;</a>
        </footer>


                </article>
                <article>
<header>
        <h1 class="entry-title">
            <a href="./pages/2019/01/algorithms-xie-tong-guo-lu-svd.html">algorithms-协同过滤SVD</a>
        </h1>
    <p class="meta">
<time datetime="2019-01-16T00:00:00+08:00" pubdate>2019-01-16 00:00</time>    </p>
</header>

    <div class="entry-content">
<div class="cell border-box-sizing code_cell rendered">
<div class="input"></div>
        <footer>
            <a rel="full-article" href="./pages/2019/01/algorithms-xie-tong-guo-lu-svd.html">Read On &crarr;</a>
        </footer>


                </article>
<div class="pagination">
    <a class="prev" href="./index4.html">&larr; Older</a>

    <a class="next" href="./index2.html">Newer &rarr;</a>
  <br />
</div>    </div>
<aside class="sidebar">
    <section>
        <h1>Recent Posts</h1>
        <ul id="recent_posts">
                <li class="post">
                    <a href="./pages/2019/01/21-a-first-look-at-a-neural-network.html">2.1-a-first-look-at-a-neural-network</a>
                </li>
                <li class="post">
                    <a href="./pages/2019/01/35-classifying-movie-reviews.html">3.5-classifying-movie-reviews</a>
                </li>
                <li class="post">
                    <a href="./pages/2019/01/36-classifying-newswires.html">3.6-classifying-newswires</a>
                </li>
                <li class="post">
                    <a href="./pages/2019/01/37-predicting-house-prices.html">3.7-predicting-house-prices</a>
                </li>
                <li class="post">
                    <a href="./pages/2019/01/44-overfitting-and-underfitting.html">4.4-overfitting-and-underfitting</a>
                </li>
        </ul>
    </section>
        <section>

            <h1>Categories</h1>
            <ul id="recent_posts">
                    <li><a href="./category/01chang yong gong ju.html">01常用工具</a></li>
                    <li><a href="./category/02.wo ai du shu.html">02.我爱读书</a></li>
                    <li><a href="./category/algorithms.html">algorithms</a></li>
                    <li><a href="./category/book.html">book</a></li>
                    <li><a href="./category/book-pydata.html">book-pydata</a></li>
                    <li><a href="./category/deep-learning-with-python.html">deep-learning-with-python</a></li>
                    <li><a href="./category/ji qi xue xi shi zhan.html">机器学习实战</a></li>
                    <li><a href="./category/ling ji chu ru men shen du xue xi.html">零基础入门深度学习</a></li>
                    <li><a href="./category/shen du xue xi.html">深度学习</a></li>
                    <li><a href="./category/shu xue ji chu.html">数学基础</a></li>
                    <li><a href="./category/tf-example.html">tf-example</a></li>
                    <li><a href="./category/tool1.html">tool1</a></li>
                    <li><a href="./category/tool2.html">tool2</a></li>
                    <li><a href="./category/tools.html">tools</a></li>
                    <li><a href="./category/tui jian xi tong.html">推荐系统</a></li>
                    <li><a href="./category/wen ben wa jue.html">文本挖掘</a></li>
            </ul>
        </section>


    <section>
        <h1>Tags</h1>
            <a href="./tag/python.html">python</a>, 
            <a href="./tag/numpy.html">numpy</a>, 
            <a href="./tag/deep-learning.html">deep-learning</a>, 
            <a href="./tag/algorithms.html">algorithms</a>, 
            <a href="./tag/wen-ben-wa-jue.html">文本挖掘</a>, 
            <a href="./tag/shu-xue-ji-chu.html">数学基础</a>, 
            <a href="./tag/nlp.html">nlp</a>, 
            <a href="./tag/tf-example.html">tf-example</a>, 
            <a href="./tag/tui-jian-xi-tong.html">推荐系统</a>, 
            <a href="./tag/tf.html">tf</a>, 
            <a href="./tag/ji-huo-han-shu.html">激活函数</a>, 
            <a href="./tag/sun-shi-han-shu.html">损失函数</a>, 
            <a href="./tag/mapreduce.html">mapreduce</a>, 
            <a href="./tag/spark.html">spark</a>, 
            <a href="./tag/handbook.html">handbook</a>, 
            <a href="./tag/matplotlib.html">matplotlib</a>, 
            <a href="./tag/scikit-learn.html">scikit-learn</a>, 
            <a href="./tag/latex.html">latex</a>, 
            <a href="./tag/pandas.html">pandas</a>, 
            <a href="./tag/jupyter.html">jupyter</a>, 
            <a href="./tag/plot.html">plot</a>, 
            <a href="./tag/pip.html">pip</a>, 
            <a href="./tag/geng-xin-suo-you-mo-kuai.html">更新所有模块</a>, 
            <a href="./tag/shen-du-xue-xi.html">深度学习</a>, 
            <a href="./tag/xun-huan-shen-jing-wang-luo.html">循环神经网络</a>, 
            <a href="./tag/pangrank.html">PangRank</a>, 
            <a href="./tag/book.html">book</a>, 
            <a href="./tag/pydata.html">pydata</a>, 
            <a href="./tag/shell.html">shell</a>, 
            <a href="./tag/pyhton.html">pyhton</a>
    </section>



    <section>
        <h1>GitHub Repos</h1>
        <ul id="gh_repos">
            <li class="loading">Status updating...</li>
        </ul>
            <a href="https://github.com/1007530194">@1007530194</a> on GitHub
        <script type="text/javascript">
            $.domReady(function () {
                if (!window.jXHR) {
                    var jxhr = document.createElement('script');
                    jxhr.type = 'text/javascript';
                    jxhr.src = './theme/js/jXHR.js';
                    var s = document.getElementsByTagName('script')[0];
                    s.parentNode.insertBefore(jxhr, s);
                }

                github.showRepos({
                    user: '1007530194',
                    count: 5,
                    skip_forks: false,
                    target: '#gh_repos'
                });
            });
        </script>
        <script src="./theme/js/github.js" type="text/javascript"></script>
    </section>

</aside>    </div>
</div>
<footer role="contentinfo"><p>
        活到老，学到老，玩到老
    <span class="credit">Powered by <a href="http://getpelican.com">Pelican</a></span>
</p></footer>
<script src="./theme/js/modernizr-2.0.js"></script>
<script src="./theme/js/ender.js"></script>
<script src="./theme/js/octopress.js" type="text/javascript"></script>
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-132396898-1', 'auto');

    ga('require', 'displayfeatures');
    ga('send', 'pageview');
    </script>
</body>
</html>