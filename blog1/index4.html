<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!-->
<html class="no-js" lang="en"><!--<![endif]-->
    <head>
<meta charset="utf-8">
<title>魑魅魍魉</title>
<meta name="author" content="niult">




<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1">


    <link href="./favicon.png" rel="icon">

<link href="./theme/css/main.css" media="screen, projection"
      rel="stylesheet" type="text/css">

<link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic"
      rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic"
      rel="stylesheet" type="text/css">
</head>

<body>
<header role="banner"><hgroup>
  <h1><a href="./">魑魅魍魉</a></h1>
</hgroup></header>
<nav role="navigation">    <ul class="subscription" data-subscription="rss">
    </ul>


<ul class="main-navigation">
            <li >
                <a href="./category/01chang yong gong ju.html">01常用工具</a>
            </li>
            <li >
                <a href="./category/02.wo ai du shu.html">02.我爱读书</a>
            </li>
            <li >
                <a href="./category/algorithms.html">Algorithms</a>
            </li>
            <li >
                <a href="./category/book.html">Book</a>
            </li>
            <li >
                <a href="./category/book-pydata.html">Book-pydata</a>
            </li>
            <li >
                <a href="./category/deep-learning-with-python.html">Deep-learning-with-python</a>
            </li>
            <li >
                <a href="./category/ji qi xue xi shi zhan.html">机器学习实战</a>
            </li>
            <li >
                <a href="./category/ling ji chu ru men shen du xue xi.html">零基础入门深度学习</a>
            </li>
            <li >
                <a href="./category/shen du xue xi.html">深度学习</a>
            </li>
            <li >
                <a href="./category/shen jing wang luo.html">神经网络</a>
            </li>
            <li >
                <a href="./category/shu xue ji chu.html">数学基础</a>
            </li>
            <li >
                <a href="./category/tf-example.html">Tf-example</a>
            </li>
            <li >
                <a href="./category/tool1.html">Tool1</a>
            </li>
            <li >
                <a href="./category/tool2.html">Tool2</a>
            </li>
            <li >
                <a href="./category/tools.html">Tools</a>
            </li>
            <li >
                <a href="./category/tui jian xi tong.html">推荐系统</a>
            </li>
            <li >
                <a href="./category/wen ben wa jue.html">文本挖掘</a>
            </li>
</ul>
</nav>
<div id="main">
    <div id="content">
    <div class="blog-index">
                <article>
<header>
        <h1 class="entry-title">
            <a href="./pages/2019/01/dnn-shen-du-shen-jing-wang-luo.html">DNN-深度神经网络</a>
        </h1>
    <p class="meta">
<time datetime="2019-01-16T00:00:00+08:00" pubdate>2019-01-16 00:00</time>    </p>
</header>

    <div class="entry-content">
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="简介">&#31616;&#20171;<a class="anchor-link" href="#简介">&#182;</a></h1><p>深度神经网络（Deep Neural Networks， 以下简称DNN）是深度学习的基础，而要理解DNN，首先我们要理解DNN模型，下面我们就对DNN的模型与前向传播算法做一个总结。</p>
<h1 id="从感知机到神经网络">&#20174;&#24863;&#30693;&#26426;&#21040;&#31070;&#32463;&#32593;&#32476;<a class="anchor-link" href="#从感知机到神经网络">&#182;</a></h1><p>在<a href="http://www.cnblogs.com/pinard/p/6042320.html">感知机原理小结</a>中，我们介绍过感知机的模型，它是一个有若干输入和一个输出的模型，如下图:</p>
<p><img src="https://images2015.cnblogs.com/blog/1042406/201702/1042406-20170220110637351-839081092.png" alt=""></p>
<p>输出和输入之间学习到一个线性关系，得到中间输出结果：$$z=\sum\limits_{i=1}^mw_ix_i + b$$</p>
<p>接着是一个神经元激活函数:</p>
<p>$$sign(z)=
\begin{cases}
-1&amp; {z&lt;0}\\
1&amp; {z\geq 0}
\end{cases}$$</p>
<p>从而得到我们想要的输出结果1或者-1。</p>
<p>这个模型只能用于二元分类，且无法学习比较复杂的非线性模型，因此在工业界无法使用。</p>
<p>而神经网络则在感知机的模型上做了扩展，总结下主要有三点：</p>
<p>1）加入了隐藏层，隐藏层可以有多层，增强模型的表达能力，如下图实例，当然增加了这么多隐藏层模型的复杂度也增加了好多。</p>
<p><img src="https://images2015.cnblogs.com/blog/1042406/201702/1042406-20170220111519210-2096738104.png" alt=""></p>
<p>2）输出层的神经元也可以不止一个输出，可以有多个输出，这样模型可以灵活的应用于分类回归，以及其他的机器学习领域比如降维和聚类等。多个神经元输出的输出层对应的一个实例如下图，输出层现在有4个神经元了。</p>
<p><img src="https://images2015.cnblogs.com/blog/1042406/201702/1042406-20170220122136538-2002639053.png" alt=""></p>
<p>3） 对激活函数做扩展，感知机的激活函数是$sign(z)$,虽然简单但是处理能力有限，因此神经网络中一般使用的其他的激活函数，比如我们在逻辑回归里面使用过的Sigmoid函数，即：$$f(z)=\frac{1}{1+e^{-z}}$$</p>
<p>还有后来出现的tanx, softmax,和ReLU等。通过使用不同的激活函数，神经网络的表达能力进一步增强。对于各种常用的激活函数，我们在后面再专门讲。</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="DNN的基本结构">DNN&#30340;&#22522;&#26412;&#32467;&#26500;<a class="anchor-link" href="#DNN的基本结构">&#182;</a></h1><p>上一节我们了解了神经网络基于感知机的扩展，而DNN可以理解为有很多隐藏层的神经网络。这个很多其实也没有什么度量标准, 多层神经网络和深度神经网络DNN其实也是指的一个东西，当然，DNN有时也叫做多层感知机（Multi-Layer perceptron,MLP）, 名字实在是多。后面我们讲到的神经网络都默认为DNN。</p>
<p>从DNN按不同层的位置划分，DNN内部的神经网络层可以分为三类，输入层，隐藏层和输出层,如下图示例，一般来说第一层是输入层，最后一层是输出层，而中间的层数都是隐藏层。</p>
<p><img src="https://images2015.cnblogs.com/blog/1042406/201702/1042406-20170220122323148-1704308672.png" alt=""></p>
<p>层与层之间是全连接的，也就是说，第i层的任意一个神经元一定与第i+1层的任意一个神经元相连。虽然DNN看起来很复杂，但是从小的局部模型来说，还是和感知机一样，即一个线性关系$z=\sum\limits w_ix_i + b$加上一个激活函数$\sigma(z)$。</p>
<p>由于DNN层数多，则我们的线性关系系数$w$和偏倚$b$的数量也就是很多了。具体的参数在DNN是如何定义的呢？</p>
<p>首先我们来看看线性关系系数$w$的定义。以下图一个三层的DNN为例，第二层的第4个神经元到第三层的第2个神经元的线性系数定义为$w_{24}^3$。上标3代表线性系数$w$所在的层数，而下标对应的是输出的第三层索引2和输入的第二层索引4。你也许会问，为什么不是$w_{42}^3$, 而是$w_{24}^3$呢？这主要是为了便于模型用于矩阵表示运算，如果是$w_{42}^3$而每次进行矩阵运算是$w^Tx+b$，需要进行转置。将输出的索引放在前面的话，则线性运算不用转置,即直接为$wx+b$。总结下，第$l-1$层的第k个神经元到第$l$层的第j个神经元的线性系数定义为$w_{jk}^l$。注意，输入层是没有$w$参数的。</p>
<p><img src="https://images2015.cnblogs.com/blog/1042406/201702/1042406-20170220134346179-1092973493.png" alt=""></p>
<p>再来看看偏倚$b$的定义。还是以这个三层的DNN为例，第二层的第三个神经元对应的偏倚定义为$b_3^{2}$。其中，上标2代表所在的层数，下标3代表偏倚所在的神经元的索引。同样的道理，第三个的第一个神经元的偏倚应该表示为$b_1^{3}$。同样的，输入层是没有偏倚参数$b$的。</p>
<p><img src="https://images2015.cnblogs.com/blog/1042406/201702/1042406-20170220135433710-1242346730.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="前向传播算法">&#21069;&#21521;&#20256;&#25773;&#31639;&#27861;<a class="anchor-link" href="#前向传播算法">&#182;</a></h1><h2 id="数学原理">&#25968;&#23398;&#21407;&#29702;<a class="anchor-link" href="#数学原理">&#182;</a></h2><p>在上一节，我们已经介绍了DNN各层线性关系系数$w$,偏倚$b$的定义。假设我们选择的激活函数是$\sigma(z)$，隐藏层和输出层的输出值为$a$，则对于下图的三层DNN,利用和感知机一样的思路，我们可以利用上一层的输出计算下一层的输出，也就是所谓的DNN前向传播算法。</p>
<p><img src="quiver-image-url/72C7151619EBDAECF8A6E0C0AD23ADE9.png" alt=""></p>
<p>对于第二层的的输出$a_1^2,a_2^2,a_3^2$，我们有：</p>
<p>$$a_1^2=\sigma(z_1^2) = \sigma(w_{11}^2x_1 + w_{12}^2x_2 + w_{13}^2x_3 + b_1^{2})$$</p>
<p>$$a_2^2=\sigma(z_2^2) = \sigma(w_{21}^2x_1 + w_{22}^2x_2 + w_{23}^2x_3 + b_2^{2})$$</p></div>
        <footer>
            <a rel="full-article" href="./pages/2019/01/dnn-shen-du-shen-jing-wang-luo.html">Read On &crarr;</a>
        </footer>


                </article>
                <article>
<header>
        <h1 class="entry-title">
            <a href="./pages/2019/01/ju-zhen-qiu-dao.html">矩阵求导</a>
        </h1>
    <p class="meta">
<time datetime="2019-01-16T00:00:00+08:00" pubdate>2019-01-16 00:00</time>    </p>
</header>

    <div class="entry-content">
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="简介">&#31616;&#20171;<a class="anchor-link" href="#简介">&#182;</a></h1><p>第一次遇见矩阵求导，大多数人都是一头雾水，而搜了维基百科看也还是云里雾里，一堆的名词和一堆的表格到底都是什么呢？这里总结了我个人的学习经验，并且通过一个例子可以让你感受如何进行矩阵求导，下次再遇到需要进行矩阵求导的地方就不会措手不及。</p>
<p>在进行概念的解说之前，首先大家需要先知道下面的这个前提：</p>
<p><a href="https://en.wikipedia.org/wiki/Matrix_calculus">https://en.wikipedia.org/wiki/Matrix_calculus</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="基本分类">&#22522;&#26412;&#20998;&#31867;<a class="anchor-link" href="#基本分类">&#182;</a></h1><h2 id="前提">&#21069;&#25552;<a class="anchor-link" href="#前提">&#182;</a></h2><p>若$x$为向量，则默认$\boldsymbol x$为列向量，$\boldsymbol x^T$为行向量
若$y$为向量，则默认$\boldsymbol y$为列向量，$\boldsymbol y^T$为行向量</p>
<h2 id="标量/向量">&#26631;&#37327;/&#21521;&#37327;<a class="anchor-link" href="#标量/向量">&#182;</a></h2><p>$$\frac {\partial y}{\partial\boldsymbol x}=
\begin{bmatrix} 
 \frac {\partial y}{\partial x_1}
\\\frac {\partial y}{\partial x_2}
\\ ...
\\\frac {\partial y}{\partial x_n}
\end{bmatrix} $$</p>
<p>$$\frac {\partial y}{\partial\boldsymbol x^T}=
\left ({\frac {\partial y^T}{\partial\boldsymbol x}} \right)^T=
\left ({\frac {\partial y}{\partial\boldsymbol x}} \right)^T=
\begin{bmatrix} 
 \frac {\partial y}{\partial x_1}
&amp;\frac {\partial y}{\partial x_2}
&amp; ...
&amp;\frac {\partial y}{\partial x_n}
\end{bmatrix} 
$$</p></div>
        <footer>
            <a rel="full-article" href="./pages/2019/01/ju-zhen-qiu-dao.html">Read On &crarr;</a>
        </footer>


                </article>
                <article>
<header>
        <h1 class="entry-title">
            <a href="./pages/2019/01/lstm-chang-duan-qi-ji-yi-wang-luo-li-lun.html">LSTM-长短期记忆网络理论</a>
        </h1>
    <p class="meta">
<time datetime="2019-01-16T00:00:00+08:00" pubdate>2019-01-16 00:00</time>    </p>
</header>

    <div class="entry-content">
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="简介">&#31616;&#20171;<a class="anchor-link" href="#简介">&#182;</a></h1><p>由于RNN也有梯度消失的问题，因此很难处理长序列的数据，大牛们对RNN做了改进，得到了RNN的特例LSTM（Long Short-Term Memory），它可以避免常规RNN的梯度消失，因此在工业界得到了广泛的应用。下面我们就对LSTM模型做一个总结。</p>
<h1 id="从RNN到LSTM">&#20174;RNN&#21040;LSTM<a class="anchor-link" href="#从RNN到LSTM">&#182;</a></h1><p>在RNN模型里，我们讲到了RNN具有如下的结构，每个序列索引位置t都有一个隐藏状态$h^{(t)}$。</p>
<p><img src="https://images2015.cnblogs.com/blog/1042406/201703/1042406-20170308133811359-125418486.png" alt=""></p>
<p>如果我们略去每层都有的$o^{(t)}, L^{(t)}, y^{(t)}$，则RNN的模型可以简化成如下图的形式：</p>
<p><img src="https://images2015.cnblogs.com/blog/1042406/201703/1042406-20170308134121500-385535455.png" alt=""></p>
<p>图中可以很清晰看出在隐藏状态$h^{(t)}$由$x^{(t)}$和$h^{(t-1)}$得到。得到$h^{(t)}$后一方面用于当前层的模型损失计算，另一方面用于计算下一层的$h^{(t+1)}$。</p>
<p>由于RNN梯度消失的问题，大牛们对于序列索引位置t的隐藏结构做了改进，可以说通过一些技巧让隐藏结构复杂了起来，来避免梯度消失的问题，这样的特殊RNN就是我们的LSTM。由于LSTM有很多的变种，这里我们以最常见的LSTM为例讲述。LSTM的结构如下图：</p>
<p><img src="https://images2015.cnblogs.com/blog/1042406/201703/1042406-20170308135057609-342574368.png" alt=""></p>
<p>可以看到LSTM的结构要比RNN的复杂的多，真佩服牛人们怎么想出来这样的结构，然后这样居然就可以解决RNN梯度消失的问题？由于LSTM怎么可以解决梯度消失是一个比较难讲的问题，我也不是很熟悉，这里就不多说，重点回到LSTM的模型本身。</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="LSTM模型结构剖析">LSTM&#27169;&#22411;&#32467;&#26500;&#21078;&#26512;<a class="anchor-link" href="#LSTM模型结构剖析">&#182;</a></h1><p>上面我们给出了LSTM的模型结构，下面我们就一点点的剖析LSTM模型在每个序列索引位置t时刻的内部结构。</p>
<p>从上图中可以看出，在每个序列索引位置t时刻向前传播的除了和RNN一样的隐藏状态$h^{(t)}$，还多了另一个隐藏状态，如图中上面的长横线。这个隐藏状态我们一般称为细胞状态(Cell State)，记为$C^{(t)}$。如下图所示：</p>
<p><img src="https://images2015.cnblogs.com/blog/1042406/201703/1042406-20170308141924453-298907713.png" alt=""></p>
<p>除了细胞状态，LSTM图中还有了很多奇怪的结构，这些结构一般称之为门控结构(Gate)。LSTM在在每个序列索引位置t的门一般包括遗忘门，输入门和输出门三种。下面我们就来研究上图中LSTM的遗忘门，输入门和输出门以及细胞状态。</p>
<h2 id="LSTM之遗忘门">LSTM&#20043;&#36951;&#24536;&#38376;<a class="anchor-link" href="#LSTM之遗忘门">&#182;</a></h2><p>遗忘门（forget gate）顾名思义，是控制是否遗忘的，在LSTM中即以一定的概率控制是否遗忘上一层的隐藏细胞状态。遗忘门子结构如下图所示：</p>
<p><img src="https://images2015.cnblogs.com/blog/1042406/201703/1042406-20170308142107484-1893679132.png" alt=""></p>
<p>图中输入的有上一序列的隐藏状态$h^{(t-1)}$和本序列数据$x^{(t)}$，通过一个激活函数，一般是sigmoid，得到遗忘门的输出$f^{(t)}$。由于sigmoid的输出$f^{(t)}$在[0,1]之间，因此这里的输出f^{(t)}代表了遗忘上一层隐藏细胞状态的概率。用数学表达式即为：$$f^{(t)} = \sigma(W_fh^{(t-1)} + U_fx^{(t)} + b_f)$$</p>
<p>其中$W_f, U_f, b_f$为线性关系的系数和偏倚，和RNN中的类似。$\sigma$为sigmoid激活函数。</p>
<h2 id="LSTM之输入门">LSTM&#20043;&#36755;&#20837;&#38376;<a class="anchor-link" href="#LSTM之输入门">&#182;</a></h2><p>输入门（input gate）负责处理当前序列位置的输入，它的子结构如下图：</p>
<p><img src="https://images2015.cnblogs.com/blog/1042406/201703/1042406-20170308143331203-803573093.png" alt=""></p>
<p>从图中可以看到输入门由两部分组成，第一部分使用了sigmoid激活函数，输出为$i^{(t)}$,第二部分使用了tanh激活函数，输出为$a^{(t)}$, 两者的结果后面会相乘再去更新细胞状态。用数学表达式即为：$$i^{(t)} = \sigma(W_ih^{(t-1)} + U_ix^{(t)} + b_i)$$$$a^{(t)} =tanh(W_ah^{(t-1)} + U_ax^{(t)} + b_a)$$</p>
<p>其中$W_i, U_i, b_i, W_a, U_a, b_a,$为线性关系的系数和偏倚，和RNN中的类似。$\sigma$为sigmoid激活函数。</p></div>
        <footer>
            <a rel="full-article" href="./pages/2019/01/lstm-chang-duan-qi-ji-yi-wang-luo-li-lun.html">Read On &crarr;</a>
        </footer>


                </article>
                <article>
<header>
        <h1 class="entry-title">
            <a href="./pages/2019/01/nlp-word2vecmo-xing.html">NLP-Word2Vec模型</a>
        </h1>
    <p class="meta">
<time datetime="2019-01-16T00:00:00+08:00" pubdate>2019-01-16 00:00</time>    </p>
</header>

    <div class="entry-content">
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="简介">&#31616;&#20171;<a class="anchor-link" href="#简介">&#182;</a></h1><p>word2vec是google在2013年推出的一个NLP工具，它的特点是将所有的词向量化，这样词与词之间就可以定量的去度量他们之间的关系，挖掘词之间的联系。虽然源码是开源的，但是谷歌的代码库国内无法访问，因此本文的讲解word2vec原理以Github上的<a href="https://github.com/tmikolov/word2vec">word2vec</a>代码为准。本文关注于word2vec的基础知识。</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="词向量基础">&#35789;&#21521;&#37327;&#22522;&#30784;<a class="anchor-link" href="#词向量基础">&#182;</a></h1><p>用词向量来表示词并不是word2vec的首创，在很久之前就出现了。最早的词向量是很冗长的，它使用是词向量维度大小为整个词汇表的大小，对于每个具体的词汇表中的词，将对应的位置置为1。比如我们有下面的5个词组成的词汇表，词"Queen"的序号为2， 那么它的词向量就是$(0,1,0,0,0)$。同样的道理，词"Woman"的词向量就是$(0,0,0,1,0)$。这种词向量的编码方式我们一般叫做1-of-N representation或者one hot representation.</p>
<p><img src="https://images2015.cnblogs.com/blog/1042406/201707/1042406-20170713145606275-2100371803.png" alt=""></p>
<p>One hot representation用来表示词向量非常简单，但是却有很多问题。最大的问题是我们的词汇表一般都非常大，比如达到百万级别，这样每个词都用百万维的向量来表示简直是内存的灾难。这样的向量其实除了一个位置是1，其余的位置全部都是0，表达的效率不高，能不能把词向量的维度变小呢？</p>
<p>Distributed representation可以解决One hot representation的问题，它的思路是通过训练，将每个词都映射到一个较短的词向量上来。所有的这些词向量就构成了向量空间，进而可以用普通的统计学的方法来研究词与词之间的关系。这个较短的词向量维度是多大呢？这个一般需要我们在训练时自己来指定。</p>
<p>比如下图我们将词汇表里的词用"Royalty","Masculinity", "Femininity"和"Age"4个维度来表示，King这个词对应的词向量可能是$(0.99, 0.99,0.05, 0.7)$。当然在实际情况中，我们并不能对词向量的每个维度做一个很好的解释。</p>
<p><img src="https://images2015.cnblogs.com/blog/1042406/201707/1042406-20170713150625759-1047275185.png" alt=""></p>
<p>有了用Distributed Representation表示的较短的词向量，我们就可以较容易的分析词之间的关系了，比如我们将词的维度降维到2维，有一个有趣的研究表明，用下图的词向量表示我们的词时，我们可以发现：$$\vec {King} - \vec {Man} + \vec {Woman} = \vec {Queen} $$</p>
<p><img src="https://images2015.cnblogs.com/blog/1042406/201707/1042406-20170713151608181-1336632086.png" alt=""></p>
<p>可见我们只要得到了词汇表里所有词对应的词向量，那么我们就可以做很多有趣的事情了。不过，怎么训练得到合适的词向量呢？一个很常见的方法是使用神经网络语言模型。</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="CBOW与Skip-Gram用于神经网络语言模型">CBOW&#19982;Skip-Gram&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#35821;&#35328;&#27169;&#22411;<a class="anchor-link" href="#CBOW与Skip-Gram用于神经网络语言模型">&#182;</a></h1><p>在word2vec出现之前，已经有用神经网络DNN来用训练词向量进而处理词与词之间的关系了。采用的方法一般是一个三层的神经网络结构（当然也可以多层），分为输入层，隐藏层和输出层(softmax层)。</p>
<p>这个模型是如何定义数据的输入和输出呢？一般分为CBOW(Continuous Bag-of-Words 与Skip-Gram两种模型。</p>
<p>CBOW模型的训练输入是某一个特征词的上下文相关的词对应的词向量，而输出就是这特定的一个词的词向量。比如下面这段话，我们的上下文大小取值为4，特定的这个词是"Learning"，也就是我们需要的输出词向量,上下文对应的词有8个，前后各4个，这8个词是我们模型的输入。由于CBOW使用的是词袋模型，因此这8个词都是平等的，也就是不考虑他们和我们关注的词之间的距离大小，只要在我们上下文之内即可。</p>
<p><img src="https://images2015.cnblogs.com/blog/1042406/201707/1042406-20170713152436931-1817493891.png" alt=""></p>
<p>这样我们这个CBOW的例子里，我们的输入是8个词向量，输出是所有词的softmax概率（训练的目标是期望训练样本特定词对应的softmax概率最大），对应的CBOW神经网络模型输入层有8个神经元，输出层有词汇表大小个神经元。隐藏层的神经元个数我们可以自己指定。通过DNN的反向传播算法，我们可以求出DNN模型的参数，同时得到所有的词对应的词向量。这样当我们有新的需求，要求出某8个词对应的最可能的输出中心词时，我们可以通过一次DNN前向传播算法并通过softmax激活函数找到概率最大的词对应的神经元即可。</p>
<p>Skip-Gram模型和CBOW的思路是反着来的，即输入是特定的一个词的词向量，而输出是特定词对应的上下文词向量。还是上面的例子，我们的上下文大小取值为4， 特定的这个词"Learning"是我们的输入，而这8个上下文词是我们的输出。</p>
<p>这样我们这个Skip-Gram的例子里，我们的输入是特定词， 输出是softmax概率排前8的8个词，对应的Skip-Gram神经网络模型输入层有1个神经元，输出层有词汇表大小个神经元。隐藏层的神经元个数我们可以自己指定。通过DNN的反向传播算法，我们可以求出DNN模型的参数，同时得到所有的词对应的词向量。这样当我们有新的需求，要求出某1个词对应的最可能的8个上下文词时，我们可以通过一次DNN前向传播算法得到概率大小排前8的softmax概率对应的神经元所对应的词即可。</p>
<p>以上就是神经网络语言模型中如何用CBOW与Skip-Gram来训练模型与得到词向量的大概过程。但是这和word2vec中用CBOW与Skip-Gram来训练模型与得到词向量的过程有很多的不同。</p>
<p>word2vec为什么 不用现成的DNN模型，要继续优化出新方法呢？最主要的问题是DNN模型的这个处理过程非常耗时。我们的词汇表一般在百万级别以上，这意味着我们DNN的输出层需要进行softmax计算各个词的输出概率的的计算量很大。有没有简化一点点的方法呢？</p></div>
        <footer>
            <a rel="full-article" href="./pages/2019/01/nlp-word2vecmo-xing.html">Read On &crarr;</a>
        </footer>


                </article>
                <article>
<header>
        <h1 class="entry-title">
            <a href="./pages/2019/01/rbm-shou-xian-bo-er-zi-man-ji-li-lun.html">RBM-受限玻尔兹曼机理论</a>
        </h1>
    <p class="meta">
<time datetime="2019-01-16T00:00:00+08:00" pubdate>2019-01-16 00:00</time>    </p>
</header>

    <div class="entry-content">
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="RBM模型">RBM&#27169;&#22411;<a class="anchor-link" href="#RBM模型">&#182;</a></h1><h2 id="模型结构">&#27169;&#22411;&#32467;&#26500;<a class="anchor-link" href="#模型结构">&#182;</a></h2><p>玻尔兹曼机是一大类的神经网络模型，但是在实际应用中使用最多的则是RBM。RBM本身模型很简单，只是一个两层的神经网络，因此严格意义上不能算深度学习的范畴。不过深度玻尔兹曼机（Deep Boltzmann Machine，以下简称DBM）可以看做是RBM的推广。理解了RBM再去研究DBM就不难了，因此本文主要关注于RBM。</p>
<p>回到RBM的结构，它是一个个两层的神经网络，如下图所示：</p>
<p><img src="https://images2015.cnblogs.com/blog/1042406/201703/1042406-20170310142842732-1310906560.jpg" alt=""></p>
<p>上面一层神经元组成隐藏层(hidden layer), 用$h$向量隐藏层神经元的值。下面一层的神经元组成可见层(visible layer),用$v$向量表示可见层神经元的值。隐藏层和可见层之间是全连接的，这点和DNN类似, 隐藏层神经元之间是独立的，可见层神经元之间也是独立的。连接权重可以用矩阵$W$表示。和DNN的区别是，RBM不区分前向和反向，可见层的状态可以作用于隐藏层，而隐藏层的状态也可以作用于可见层。隐藏层的偏倚系数是向量$b$,而可见层的偏倚系数是向量$a$。</p>
<p>常用的RBM一般是二值的，即不管是隐藏层还是可见层，它们的神经元的取值只为0或者1。本文只讨论二值RBM。</p>
<p>总结下RBM模型结构的结构：主要是权重矩阵$W$, 偏倚系数向量$a$和$b$，隐藏层神经元状态向量$h$和可见层神经元状态向量$v$。</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="概率分布">&#27010;&#29575;&#20998;&#24067;<a class="anchor-link" href="#概率分布">&#182;</a></h2><p>RBM是基于基于能量的概率分布模型。怎么理解呢？分两部分理解，第一部分是能量函数，第二部分是基于能量函数的概率分布函数。</p>
<p>对于给定的状态向量$h$和$v$，则RBM当前的能量函数可以表示为：$$E(v,h) = -a^Tv - b^Th - h^TWv $$</p>
<p>有了能量函数，则我们可以定义RBM的状态为给定$v,h$的概率分布为：$$P(v,h) = \frac{1}{Z}e^{-E(v,h)}$$</p>
<p>其中$Z$为归一化因子，类似于softmax中的归一化因子，表达式为：$$Z = \sum\limits_{v,h}e^{-E(v,h)}$$</p>
<p>有了概率分布，我们现在来看条件分布$P(h|v)$:
 \begin{align} 
 P(h|v) &amp; = \frac{P(h,v)}{P(v)} \\&amp; = \frac{1}{P(v)}\frac{1}{Z}exp\{a^Tv + b^Th + h^TWv\} \\&amp; = \frac{1}{Z'}exp\{b^Th + h^TWv\} \\&amp; = \frac{1}{Z'}exp\{\sum\limits_{j=1}^{n_h}(b_j^Th_j + h_j^TW_{j,:}v)\} \\&amp; = \frac{1}{Z'} \prod\limits_{j=1}^{n_h}exp\{b_j^Th_j + h_j^TW_{j,:}v\} 
 \end{align}</p></div>
        <footer>
            <a rel="full-article" href="./pages/2019/01/rbm-shou-xian-bo-er-zi-man-ji-li-lun.html">Read On &crarr;</a>
        </footer>


                </article>
                <article>
<header>
        <h1 class="entry-title">
            <a href="./pages/2019/01/rbm-shou-xian-bo-er-zi-man-ji-shi-jian.html">RBM-受限玻尔兹曼机实践</a>
        </h1>
    <p class="meta">
<time datetime="2019-01-16T00:00:00+08:00" pubdate>2019-01-16 00:00</time>    </p>
</header>

    <div class="entry-content">
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="实例一">&#23454;&#20363;&#19968;<a class="anchor-link" href="#实例一">&#182;</a></h1><p>很多人讲RBM都要从能量函数讲起，由能量最低导出极小化目标函数（你听说过最常见的建立目标函数的方法可能是最小化平方误差或者最大化似然函数），然后用梯度下降法求解，得到网络参数。Introduction to Restricted Boltzmann Machines这篇博客没有遵循这种套路来讲RBM，它直接给RBM网络权重的训练方法，讲得浅显易懂，清新脱俗。本文只是对英文版的翻译。</p>
<p>在基于LFM(Latent Factor Model)的推荐算法一文中我们介绍了用因子分析法来做推荐。比如用户购买了《推荐系统实践》、《用Python做数据分析》，背后的隐藏因子是数据挖掘；用户购买了《一课经济学》、《郎咸平说》，背后的隐藏因子是经济学。因子分析法就是找到用户对各个隐藏因子的喜好程度$U=(uf_1,uf_2,...,uf_n)$，以及商品在各个隐藏因子上的概率分布$I=(if_1,if_2,...,if_n)$，然后两个向量做乘法即得到用户对商品的喜好程度。RBM可以理解为一种二值化的因子分析法（当然RBM还有其他的理解方式）。先来一张图看看RBM的网络结构。</p>
<p>它只有两层：可视层和隐藏层，两层之间是全连接。另有一个偏置单元，跟所有的可视单元和隐藏单元都有连接。可视层之间、隐藏层之间无连接。
所有连接都是双向的，并且是带权重的。
所有神经元的状态只有0和1两种。
RBM是一个随机网络，即所有神经元以一个概率值选择状态为0还是1。偏置单元是个例外，它总为1。偏置单元用来反应商品固有的受欢迎程度，所谓“固有”就是跟外界无关，反应到RBM网络里面就是隐藏单元的状态并不是完全由可视层决定的，也由隐藏层神经元自身固有的一些因素决定，这些固有的因素就由偏置单元来承载。反过来对于可视层也一样，可视层神经元固有的受欢迎程度由偏置单元来承载。我们在带偏置的LFM中也说明了偏置的作用。
RBM的运作方式
可视层的神经元用$x_i$表示，隐藏层神经元用$x_j$表示，它们之间的权重用$w_{ij}$表示，可视层神经元个数为$m$，隐藏层神经元个数为$n$。当给定可视层状态后，用下式更新隐藏层的状态。</p>
<p>\begin{equation}net_j=\sum_{i=0}^m{x_i{w_{ij}}}\end{equation}$x_0$是偏置单元，总为1</p>
<p>\begin{equation}prob(j)=sigmoid(net_j)=\frac{1}{1+e^{-net_j}}\end{equation}</p>
<p>$x_j$以概率$prob(j)$取1，以概率$1-prob(j)$取0。</p>
<p>$sigmoid$函数关于(0,0.5)这一点中心对称，$x$为正时$sigmoid(x)&gt;0.5$，$x\to\infty$时$sigmoid(x)\to{1}$。</p>
<p>根据隐藏层求可视层方式雷同，就不写公式了。</p>
<p>对于推荐系统来说，我们知道用户购买了哪些商品，将对应的可视层神经元置为1，其他置为0，求出隐藏层状态，由隐藏层再返回来求可视层状态，这个时候可视层哪些神经元为1我们就把相应有商品推荐给用户。</p>
<p>权重学习方法
训练RBM网络就是训练权重$w_{ij}$。首先随机初始化$w_{ij}$，然后每一次拿一个样本（即可视层是已知的）经历下面的步骤。</p>
<p>由可视层的$x_i$算出隐藏层的$x_j$，令$w_{ij}$的正向梯度为\begin{equation}positive(w_{ij})=x_i*{x_j}\end{equation}
由隐藏层$x_j$再来反向计算$x’_i$，注意此时算出的$x’_i$跟原先的$x_i$已经不一样了，令$w_{ij}$的负向梯度为\begin{equation}negative(w_{ij})=x'_i*{x_j}\end{equation}
更新权重\begin{equation}w_{ij}=w_{ij}+\alpha*(positive(w_{ij})-negative(w_{ij}))\end{equation}
我们不去深究为什么正向梯度和负向梯度是这样一个公式。上述学习方式叫对比散度(contrastive divergence)法。</p>
<p>循环拿样本去训练网络，不停迭代，直到收敛（即$x_i'$和$x_i$很接近）。</p>
<p>实践中的优化
上面讲述中我们拿$x_i$去RBM网络中返回一次得到$x_i'$，然后就开始计算$negative(w_{ij})$，改进方法是多往返几次后再计算$negative(w_{ij})$。
计算$positive(w_{ij})$时用$prob(i)*prob(j)$，而非$x_i*{x_j}$。$negative(w_{ij})$同样。
加正则项，对较大的权重$w_{ij}$进行惩罚。
更新$w_{ij}$时加动量向，即本次前进的方向是本次的梯度与上次迭代中梯度的线性加权。
每次调整权重时使用一批样本，而非不一个样本。虽然计算结果是一样的，但由于numpy对矩阵乘法做了加速优化，比逐个计算向量乘法要快。</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input"></div>
        <footer>
            <a rel="full-article" href="./pages/2019/01/rbm-shou-xian-bo-er-zi-man-ji-shi-jian.html">Read On &crarr;</a>
        </footer>


                </article>
                <article>
<header>
        <h1 class="entry-title">
            <a href="./pages/2019/01/rnn-di-gui-shen-jing-wang-luo.html">RNN-递归神经网络</a>
        </h1>
    <p class="meta">
<time datetime="2019-01-16T00:00:00+08:00" pubdate>2019-01-16 00:00</time>    </p>
</header>

    <div class="entry-content">
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="RNN概述">RNN&#27010;&#36848;<a class="anchor-link" href="#RNN概述">&#182;</a></h1><p>在前面讲到的DNN和CNN中，训练样本的输入和输出是比较的确定的。但是有一类问题DNN和CNN不好解决，就是训练样本输入是连续的序列,且序列的长短不一，比如基于时间的序列：一段段连续的语音，一段段连续的手写文字。这些序列比较长，且长度不一，比较难直接的拆分成一个个独立的样本来通过DNN/CNN进行训练。</p>
<p>而对于这类问题，RNN则比较的擅长。那么RNN是怎么做到的呢？RNN假设我们的样本是基于序列的。比如是从序列索引1到序列索引$\tau$的。对于这其中的任意序列索引号$t$,它对应的输入是对应的样本序列中的$x^{(t)}$。而模型在序列索引号$t$位置的隐藏状态$h^{(t)}$，则由$x^{(t)}$和在$t-1$位置的隐藏状态$h^{(t-1)}$共同决定。在任意序列索引号$t$，我们也有对应的模型预测输出$o^{(t)}$。通过预测输出$o^{(t)}$和训练序列真实输出$y^{(t)}$,以及损失函数$L^{(t)}$，我们就可以用DNN类似的方法来训练模型，接着用来预测测试序列中的一些位置的输出。</p>
<p>下面我们来看看RNN的模型。</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="RNN模型">RNN&#27169;&#22411;<a class="anchor-link" href="#RNN模型">&#182;</a></h1><p>RNN模型有比较多的变种，这里介绍最主流的RNN模型结构如下：</p>
<p><img src="https://images2015.cnblogs.com/blog/1042406/201703/1042406-20170306142253375-175971779.png" alt=""></p>
<p>上图中左边是RNN模型没有按时间展开的图，如果按时间序列展开，则是上图中的右边部分。我们重点观察右边部分的图。</p>
<p>这幅图描述了在序列索引号$t$附近RNN的模型。其中：</p>
<p>1）$x^{(t)}$代表在序列索引号$t$时训练样本的输入。同样的，$x^{(t-1)}$和$x^{(t+1)}$代表在序列索引号$t-1$和$t+1$时训练样本的输入。</p>
<p>2）$h^{(t)}$代表在序列索引号$t$时模型的隐藏状态。$h^{(t)}$由$x^{(t)}$和$h^{(t-1)}$共同决定。</p>
<p>3）$o^{(t)}$代表在序列索引号$t$时模型的输出。$o^{(t)}$只由模型当前的隐藏状态$h^{(t)}$决定。</p>
<p>4）$L^{(t)}$代表在序列索引号$t$时模型的损失函数。</p>
<p>5）$y^{(t)}$代表在序列索引号$t$时训练样本序列的真实输出。</p>
<p>6）$U,W,V$这三个矩阵是我们的模型的线性关系参数，它在整个RNN网络中是共享的，这点和DNN很不相同。 也正因为是共享了，它体现了RNN的模型的“循环反馈”的思想。</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="前向传播算法">&#21069;&#21521;&#20256;&#25773;&#31639;&#27861;<a class="anchor-link" href="#前向传播算法">&#182;</a></h2><p>有了上面的模型，RNN的前向传播算法就很容易得到了。</p>
<p>对于任意一个序列索引号$t$，我们隐藏状态$h^{(t)}$由$x^{(t)}$和$h^{(t-1)}$得到：$$h^{(t)} = \sigma(z^{(t)}) = \sigma(Ux^{(t)} + Wh^{(t-1)} +b )$$</p>
<p>其中$\sigma$为RNN的激活函数，一般为$tanh$, $b$为线性关系的偏倚。</p>
<p>序列索引号$t$时模型的输出$o^{(t)}$的表达式比较简单：$$o^{(t)} = Vh^{(t)} +c $$</p>
<p>在最终在序列索引号$t$时我们的预测输出为:$$\hat{y}^{(t)} = \sigma(o^{(t)})$$</p>
<p>通常由于RNN是识别类的分类模型，所以上面这个激活函数一般是softmax。</p>
<p>通过损失函数$L^{(t)}$，比如对数似然损失函数，我们可以量化模型在当前位置的损失，即$\hat{y}^{(t)}$和$y^{(t)}$的差距。</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="反向传播算法推导">&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#25512;&#23548;<a class="anchor-link" href="#反向传播算法推导">&#182;</a></h2><p>有了RNN前向传播算法的基础，就容易推导出RNN反向传播算法的流程了。RNN反向传播算法的思路和DNN是一样的，即通过梯度下降法一轮轮的迭代，得到合适的RNN模型参数$U,W,V,b,c$。由于我们是基于时间反向传播，所以RNN的反向传播有时也叫做BPTT(back-propagation through time)。当然这里的BPTT和DNN也有很大的不同点，即这里所有的$U,W,V,b,c$在序列的各个位置是共享的，反向传播时我们更新的是相同的参数。</p>
<p>为了简化描述，这里的损失函数我们为对数损失函数，输出的激活函数为softmax函数，隐藏层的激活函数为tanh函数。</p>
<p>对于RNN，由于我们在序列的每个位置都有损失函数，因此最终的损失$L$为：$$L = \sum\limits_{t=1}^{\tau}L^{(t)}$$</p>
<p>其中$V,c,$的梯度计算是比较简单的：$$\frac{\partial L}{\partial c} = \sum\limits_{t=1}^{\tau}\frac{\partial L^{(t)}}{\partial c} = \sum\limits_{t=1}^{\tau}\frac{\partial L^{(t)}}{\partial o^{(t)}} \frac{\partial o^{(t)}}{\partial c} = \sum\limits_{t=1}^{\tau}\hat{y}^{(t)} - y^{(t)}$$$$\frac{\partial L}{\partial V} =\sum\limits_{t=1}^{\tau}\frac{\partial L^{(t)}}{\partial V} = \sum\limits_{t=1}^{\tau}\frac{\partial L^{(t)}}{\partial o^{(t)}} \frac{\partial o^{(t)}}{\partial V} = \sum\limits_{t=1}^{\tau}(\hat{y}^{(t)} - y^{(t)}) (h^{(t)})^T$$</p></div>
        <footer>
            <a rel="full-article" href="./pages/2019/01/rnn-di-gui-shen-jing-wang-luo.html">Read On &crarr;</a>
        </footer>


                </article>
                <article>
<header>
        <h1 class="entry-title">
            <a href="./pages/2019/01/shu-xue-ji-chu-zhu-cheng-fen-fen-xi.html">数学基础-主成分分析</a>
        </h1>
    <p class="meta">
<time datetime="2019-01-16T00:00:00+08:00" pubdate>2019-01-16 00:00</time>    </p>
</header>

    <div class="entry-content">
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="PCA简介">PCA&#31616;&#20171;<a class="anchor-link" href="#PCA简介">&#182;</a></h1><h2 id="相关背景">&#30456;&#20851;&#32972;&#26223;<a class="anchor-link" href="#相关背景">&#182;</a></h2><p>上完陈恩红老师的《机器学习与知识发现》和季海波老师的《矩阵代数》两门课之后，颇有体会。最近在做主成分分析和奇异值分解方面的项目，所以记录一下心得体会。</p>
<p>在许多领域的研究与应用中，往往需要对反映事物的多个变量进行大量的观测，收集大量数据以便进行分析寻找规律。多变量大样本无疑会为研究和应用提供了丰富的信息，但也在一定程度上增加了数据采集的工作量，更重要的是在多数情况下，许多变量之间可能存在相关性，从而增加了问题分析的复杂性，同时对分析带来不便。如果分别对每个指标进行分析，分析往往是孤立的，而不是综合的。盲目减少指标会损失很多信息，容易产生错误的结论。</p>
<p>因此需要找到一个合理的方法，在减少需要分析的指标同时，尽量减少原指标包含信息的损失，以达到对所收集数据进行全面分析的目的。由于各变量间存在一定的相关关系，因此有可能用较少的综合指标分别综合存在于各变量中的各类信息。主成分分析与因子分析就属于这类降维的方法。</p>
<p><a href="http://blog.csdn.net/zhongkelee/article/details/44064401">参考文献</a></p>
<h2 id="问题描述">&#38382;&#39064;&#25551;&#36848;<a class="anchor-link" href="#问题描述">&#182;</a></h2><p>下表1是某些学生的语文、数学、物理、化学成绩统计：</p>
<p><img src="https://raw.githubusercontent.com/1007530194/data/master/image/blog/20190101/B9DA6A4845C16156E47AD335D18EA6E0.jpg?raw=true" alt=""></p>
<p>首先，假设这些科目成绩不相关，也就是说某一科目考多少分与其他科目没有关系。那么一眼就能看出来，数学、物理、化学这三门课的成绩构成了这组数据的主成分（很显然，数学作为第一主成分，因为数学成绩拉的最开）。为什么一眼能看出来？因为坐标轴选对了！下面再看一组学生的数学、物理、化学、语文、历史、英语成绩统计，见表2，还能不能一眼看出来：</p>
<p><img src="https://raw.githubusercontent.com/1007530194/data/master/image/blog/20190101/69D88091D516A0064551D8F2881A2EDE.jpg?raw=true" alt=""></p>
<p>数据太多了，以至于看起来有些凌乱！也就是说，无法直接看出这组数据的主成分，因为在坐标系下这组数据分布的很散乱。究其原因，是因为无法拨开遮住肉眼的迷雾~如果把这些数据在相应的空间中表示出来，也许你就能换一个观察角度找出主成分。如下图1所示：</p>
<p><img src="https://raw.githubusercontent.com/1007530194/data/master/image/blog/20190101/DB0C770D8DD55E5A433B6306D7085BF3.jpg?raw=true" alt=""></p>
<p>但是，对于更高维的数据，能想象其分布吗？就算能描述分布，如何精确地找到这些主成分的轴？如何衡量你提取的主成分到底占了整个数据的多少信息？所以，我们就要用到主成分分析的处理方法。</p>
<h2 id="数据降维">&#25968;&#25454;&#38477;&#32500;<a class="anchor-link" href="#数据降维">&#182;</a></h2><p>为了说明什么是数据的主成分，先从数据降维说起。数据降维是怎么回事儿？假设三维空间中有一系列点，这些点分布在一个过原点的斜面上，如果你用自然坐标系x,y,z这三个轴来表示这组数据的话，需要使用三个维度，而事实上，这些点的分布仅仅是在一个二维的平面上，那么，问题出在哪里？如果你再仔细想想，能不能把x,y,z坐标系旋转一下，使数据所在平面与x,y平面重合？这就对了！如果把旋转后的坐标系记为x',y',z'，那么这组数据的表示只用x'和y'两个维度表示即可！当然了，如果想恢复原来的表示方式，那就得把这两个坐标之间的变换矩阵存下来。这样就能把数据维度降下来了！但是，我们要看到这个过程的本质，如果把这些数据按行或者按列排成一个矩阵，那么这个矩阵的秩就是2！这些数据之间是有相关性的，这些数据构成的过原点的向量的最大线性无关组包含2个向量，这就是为什么一开始就假设平面过原点的原因！那么如果平面不过原点呢？这就是数据中心化的缘故！将坐标原点平移到数据中心，这样原本不相关的数据在这个新坐标系中就有相关性了！有趣的是，三点一定共面，也就是说三维空间中任意三点中心化后都是线性相关的，一般来讲n维空间中的n个点一定能在一个n-1维子空间中分析！</p>
<p>上一段文字中，认为把数据降维后并没有丢弃任何东西，因为这些数据在平面以外的第三个维度的分量都为0。现在，假设这些数据在z'轴有一个很小的抖动，那么我们仍然用上述的二维表示这些数据，理由是我们可以认为这两个轴的信息是数据的主成分，而这些信息对于我们的分析已经足够了，z'轴上的抖动很有可能是噪声，也就是说本来这组数据是有相关性的，噪声的引入，导致了数据不完全相关，但是，这些数据在z'轴上的分布与原点构成的夹角非常小，也就是说在z'轴上有很大的相关性，综合这些考虑，就可以认为数据在x',y' 轴上的投影构成了数据的主成分！</p>
<p>课堂上老师谈到的特征选择的问题，其实就是要剔除的特征主要是和类标签无关的特征。而这里的特征很多是和类标签有关的，但里面存在噪声或者冗余。在这种情况下，需要一种特征降维的方法来减少特征数，减少噪音和冗余，减少过度拟合的可能性。</p>
<p>PCA的思想是将n维特征映射到k维上（k\&lt;n），这k维是全新的正交特征。这k维特征称为主成分，是重新构造出来的k维特征，而不是简单地从n维特征中去除其余n-k维特征。</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="PCA实例">PCA&#23454;&#20363;<a class="anchor-link" href="#PCA实例">&#182;</a></h1><p>现在假设有一组数据如下：</p>
<p><img src="https://raw.githubusercontent.com/1007530194/data/master/image/blog/20190101/75D505A58D5B23BD4ED7207C087B5F04.jpg?raw=true" alt=""></p>
<p>行代表了样例，列代表特征，这里有10个样例，每个样例两个特征。可以这样认为，有10篇文档，x是10篇文档中“learn”出现的TF-IDF，y是10篇文档中“study”出现的TF-IDF。
<strong>第一步</strong>，分别求x和y的平均值，然后对于所有的样例，都减去对应的均值。这里x的均值是1.81，y的均值是1.91，那么一个样例减去均值后即为（0.69,0.49），得到</p>
<p><img src="https://raw.githubusercontent.com/1007530194/data/master/image/blog/20190101/8D90FF4FA2A8A73A23B6D3C61E0892AA.jpg?raw=true" alt=""></p>
<p><strong>第二步</strong>，求特征协方差矩阵，如果数据是3维，那么协方差矩阵是</p>
<p>$$C =\begin{pmatrix}    cov(x,x) &amp; cov(x,y) &amp; cov(x,z) \\ cov(y,x) &amp; cov(y,y) &amp; cov(y,z) \\  cov(z,x) &amp; cov(z,y) &amp; cov(z,z)\end{pmatrix}$$</p>
<p>这里只有$x$和$y$，求解得</p>
<p>$$cov =\begin{pmatrix}  0.6166 &amp; 0.6154 \\ 0.6154 &amp; 0.7166 \end{pmatrix}$$</p></div>
        <footer>
            <a rel="full-article" href="./pages/2019/01/shu-xue-ji-chu-zhu-cheng-fen-fen-xi.html">Read On &crarr;</a>
        </footer>


                </article>
                <article>
<header>
        <h1 class="entry-title">
            <a href="./pages/2019/01/tf-example-0101-helloworld.html">tf-example-01.01-helloworld</a>
        </h1>
    <p class="meta">
<time datetime="2019-01-16T00:00:00+08:00" pubdate>2019-01-16 00:00</time>    </p>
</header>

    <div class="entry-content">
<div class="cell border-box-sizing code_cell rendered">
<div class="input"></div>
        <footer>
            <a rel="full-article" href="./pages/2019/01/tf-example-0101-helloworld.html">Read On &crarr;</a>
        </footer>


                </article>
                <article>
<header>
        <h1 class="entry-title">
            <a href="./pages/2019/01/tf-example-0102-basic-eager-api.html">tf-example-01.02-basic-eager-api</a>
        </h1>
    <p class="meta">
<time datetime="2019-01-16T00:00:00+08:00" pubdate>2019-01-16 00:00</time>    </p>
</header>

    <div class="entry-content">
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Basic-introduction-to-TensorFlow's-Eager-API">Basic introduction to TensorFlow's Eager API<a class="anchor-link" href="#Basic-introduction-to-TensorFlow's-Eager-API">&#182;</a></h1><p>A simple introduction to get started with TensorFlow's Eager API.</p>
<ul>
<li>Author: Aymeric Damien</li>
<li>Project: <a href="https://github.com/aymericdamien/TensorFlow-Examples/">https://github.com/aymericdamien/TensorFlow-Examples/</a></li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="What-is-TensorFlow's-Eager-API-?">What is TensorFlow's Eager API ?<a class="anchor-link" href="#What-is-TensorFlow's-Eager-API-?">&#182;</a></h3><p><em>Eager execution is an imperative, define-by-run interface where operations are
executed immediately as they are called from Python. This makes it easier to
get started with TensorFlow, and can make research and development more
intuitive. A vast majority of the TensorFlow API remains the same whether eager
execution is enabled or not. As a result, the exact same code that constructs
TensorFlow graphs (e.g. using the layers API) can be executed imperatively
by using eager execution. Conversely, most models written with Eager enabled
can be converted to a graph that can be further optimized and/or extracted
for deployment in production without changing code. - Rajat Monga</em></div>
        <footer>
            <a rel="full-article" href="./pages/2019/01/tf-example-0102-basic-eager-api.html">Read On &crarr;</a>
        </footer>


                </article>
                <article>
<header>
        <h1 class="entry-title">
            <a href="./pages/2019/01/tf-example-0103-basic-operations.html">tf-example-01.03-basic-operations</a>
        </h1>
    <p class="meta">
<time datetime="2019-01-16T00:00:00+08:00" pubdate>2019-01-16 00:00</time>    </p>
</header>

    <div class="entry-content">
<div class="cell border-box-sizing code_cell rendered">
<div class="input"></div>
        <footer>
            <a rel="full-article" href="./pages/2019/01/tf-example-0103-basic-operations.html">Read On &crarr;</a>
        </footer>


                </article>
                <article>
<header>
        <h1 class="entry-title">
            <a href="./pages/2019/01/tf-example-0201-linear_regression_eager_api.html">tf-example-02.01-linear_regression_eager_api</a>
        </h1>
    <p class="meta">
<time datetime="2019-01-16T00:00:00+08:00" pubdate>2019-01-16 00:00</time>    </p>
</header>

    <div class="entry-content">
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Linear-Regression-with-Eager-API">Linear Regression with Eager API<a class="anchor-link" href="#Linear-Regression-with-Eager-API">&#182;</a></h1><p>A linear regression implemented using TensorFlow's Eager API.</p>
<ul>
<li>Author: Aymeric Damien</li>
<li>Project: <a href="https://github.com/aymericdamien/TensorFlow-Examples/">https://github.com/aymericdamien/TensorFlow-Examples/</a></li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input"></div>
        <footer>
            <a rel="full-article" href="./pages/2019/01/tf-example-0201-linear_regression_eager_api.html">Read On &crarr;</a>
        </footer>


                </article>
                <article>
<header>
        <h1 class="entry-title">
            <a href="./pages/2019/01/tf-example-0202-linear_regression.html">tf-example-02.02-linear_regression</a>
        </h1>
    <p class="meta">
<time datetime="2019-01-16T00:00:00+08:00" pubdate>2019-01-16 00:00</time>    </p>
</header>

    <div class="entry-content">
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Linear-Regression-Example">Linear Regression Example<a class="anchor-link" href="#Linear-Regression-Example">&#182;</a></h1><p>A linear regression learning algorithm example using TensorFlow library.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input"></div>
        <footer>
            <a rel="full-article" href="./pages/2019/01/tf-example-0202-linear_regression.html">Read On &crarr;</a>
        </footer>


                </article>
                <article>
<header>
        <h1 class="entry-title">
            <a href="./pages/2019/01/tf-example-0203-logistic_regression_eager_api.html">tf-example-02.03-logistic_regression_eager_api</a>
        </h1>
    <p class="meta">
<time datetime="2019-01-16T00:00:00+08:00" pubdate>2019-01-16 00:00</time>    </p>
</header>

    <div class="entry-content">
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Logistic-Regression-with-Eager-API">Logistic Regression with Eager API<a class="anchor-link" href="#Logistic-Regression-with-Eager-API">&#182;</a></h1><p>A logistic regression implemented using TensorFlow's Eager API.</p>
<ul>
<li>Author: Aymeric Damien</li>
<li>Project: <a href="https://github.com/aymericdamien/TensorFlow-Examples/">https://github.com/aymericdamien/TensorFlow-Examples/</a></li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="MNIST-Dataset-Overview">MNIST Dataset Overview<a class="anchor-link" href="#MNIST-Dataset-Overview">&#182;</a></h2><p>This example is using MNIST handwritten digits. The dataset contains 60,000 examples for training and 10,000 examples for testing. The digits have been size-normalized and centered in a fixed-size image (28x28 pixels) with values from 0 to 1. For simplicity, each image has been flattened and converted to a 1-D numpy array of 784 features (28*28).</p></div>
        <footer>
            <a rel="full-article" href="./pages/2019/01/tf-example-0203-logistic_regression_eager_api.html">Read On &crarr;</a>
        </footer>


                </article>
                <article>
<header>
        <h1 class="entry-title">
            <a href="./pages/2019/01/tf-example-0204-nearest_neighbor.html">tf-example-02.04-nearest_neighbor</a>
        </h1>
    <p class="meta">
<time datetime="2019-01-16T00:00:00+08:00" pubdate>2019-01-16 00:00</time>    </p>
</header>

    <div class="entry-content">
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Nearest-Neighbor-Example">Nearest Neighbor Example<a class="anchor-link" href="#Nearest-Neighbor-Example">&#182;</a></h1><p>A nearest neighbor learning algorithm example using TensorFlow library.
This example is using the MNIST database of handwritten digits (<a href="http://yann.lecun.com/exdb/mnist/">http://yann.lecun.com/exdb/mnist/</a>)</p>
<ul>
<li>Author: Aymeric Damien</li>
<li>Project: <a href="https://github.com/aymericdamien/TensorFlow-Examples/">https://github.com/aymericdamien/TensorFlow-Examples/</a></li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input"></div>
        <footer>
            <a rel="full-article" href="./pages/2019/01/tf-example-0204-nearest_neighbor.html">Read On &crarr;</a>
        </footer>


                </article>
<div class="pagination">
    <a class="prev" href="./index5.html">&larr; Older</a>

    <a class="next" href="./index3.html">Newer &rarr;</a>
  <br />
</div>    </div>
<aside class="sidebar">
    <section>
        <h1>Recent Posts</h1>
        <ul id="recent_posts">
                <li class="post">
                    <a href="./pages/2019/01/21-a-first-look-at-a-neural-network.html">2.1-a-first-look-at-a-neural-network</a>
                </li>
                <li class="post">
                    <a href="./pages/2019/01/35-classifying-movie-reviews.html">3.5-classifying-movie-reviews</a>
                </li>
                <li class="post">
                    <a href="./pages/2019/01/36-classifying-newswires.html">3.6-classifying-newswires</a>
                </li>
                <li class="post">
                    <a href="./pages/2019/01/37-predicting-house-prices.html">3.7-predicting-house-prices</a>
                </li>
                <li class="post">
                    <a href="./pages/2019/01/44-overfitting-and-underfitting.html">4.4-overfitting-and-underfitting</a>
                </li>
        </ul>
    </section>
        <section>

            <h1>Categories</h1>
            <ul id="recent_posts">
                    <li><a href="./category/01chang yong gong ju.html">01常用工具</a></li>
                    <li><a href="./category/02.wo ai du shu.html">02.我爱读书</a></li>
                    <li><a href="./category/algorithms.html">algorithms</a></li>
                    <li><a href="./category/book.html">book</a></li>
                    <li><a href="./category/book-pydata.html">book-pydata</a></li>
                    <li><a href="./category/deep-learning-with-python.html">deep-learning-with-python</a></li>
                    <li><a href="./category/ji qi xue xi shi zhan.html">机器学习实战</a></li>
                    <li><a href="./category/ling ji chu ru men shen du xue xi.html">零基础入门深度学习</a></li>
                    <li><a href="./category/shen du xue xi.html">深度学习</a></li>
                    <li><a href="./category/shen jing wang luo.html">神经网络</a></li>
                    <li><a href="./category/shu xue ji chu.html">数学基础</a></li>
                    <li><a href="./category/tf-example.html">tf-example</a></li>
                    <li><a href="./category/tool1.html">tool1</a></li>
                    <li><a href="./category/tool2.html">tool2</a></li>
                    <li><a href="./category/tools.html">tools</a></li>
                    <li><a href="./category/tui jian xi tong.html">推荐系统</a></li>
                    <li><a href="./category/wen ben wa jue.html">文本挖掘</a></li>
            </ul>
        </section>


    <section>
        <h1>Tags</h1>
            <a href="./tag/python.html">python</a>, 
            <a href="./tag/numpy.html">numpy</a>, 
            <a href="./tag/deep-learning.html">deep-learning</a>, 
            <a href="./tag/algorithms.html">algorithms</a>, 
            <a href="./tag/wen-ben-wa-jue.html">文本挖掘</a>, 
            <a href="./tag/shen-jing-wang-luo.html">神经网络</a>, 
            <a href="./tag/shu-xue-ji-chu.html">数学基础</a>, 
            <a href="./tag/nlp.html">nlp</a>, 
            <a href="./tag/tf-example.html">tf-example</a>, 
            <a href="./tag/tui-jian-xi-tong.html">推荐系统</a>, 
            <a href="./tag/tf.html">tf</a>, 
            <a href="./tag/ji-huo-han-shu.html">激活函数</a>, 
            <a href="./tag/mapreduce.html">mapreduce</a>, 
            <a href="./tag/spark.html">spark</a>, 
            <a href="./tag/handbook.html">handbook</a>, 
            <a href="./tag/matplotlib.html">matplotlib</a>, 
            <a href="./tag/scikit-learn.html">scikit-learn</a>, 
            <a href="./tag/latex.html">latex</a>, 
            <a href="./tag/pandas.html">pandas</a>, 
            <a href="./tag/jupyter.html">jupyter</a>, 
            <a href="./tag/plot.html">plot</a>, 
            <a href="./tag/pip.html">pip</a>, 
            <a href="./tag/geng-xin-suo-you-mo-kuai.html">更新所有模块</a>, 
            <a href="./tag/shen-du-xue-xi.html">深度学习</a>, 
            <a href="./tag/xun-huan-shen-jing-wang-luo.html">循环神经网络</a>, 
            <a href="./tag/pangrank.html">PangRank</a>, 
            <a href="./tag/book.html">book</a>, 
            <a href="./tag/pydata.html">pydata</a>, 
            <a href="./tag/shell.html">shell</a>, 
            <a href="./tag/pyhton.html">pyhton</a>
    </section>



    <section>
        <h1>GitHub Repos</h1>
        <ul id="gh_repos">
            <li class="loading">Status updating...</li>
        </ul>
            <a href="https://github.com/1007530194">@1007530194</a> on GitHub
        <script type="text/javascript">
            $.domReady(function () {
                if (!window.jXHR) {
                    var jxhr = document.createElement('script');
                    jxhr.type = 'text/javascript';
                    jxhr.src = './theme/js/jXHR.js';
                    var s = document.getElementsByTagName('script')[0];
                    s.parentNode.insertBefore(jxhr, s);
                }

                github.showRepos({
                    user: '1007530194',
                    count: 5,
                    skip_forks: false,
                    target: '#gh_repos'
                });
            });
        </script>
        <script src="./theme/js/github.js" type="text/javascript"></script>
    </section>

</aside>    </div>
</div>
<footer role="contentinfo"><p>
        活到老，学到老，玩到老
    <span class="credit">Powered by <a href="http://getpelican.com">Pelican</a></span>
</p></footer>
<script src="./theme/js/modernizr-2.0.js"></script>
<script src="./theme/js/ender.js"></script>
<script src="./theme/js/octopress.js" type="text/javascript"></script>
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-132396898-1', 'auto');

    ga('require', 'displayfeatures');
    ga('send', 'pageview');
    </script>
</body>
</html>